/// Generated by `cargo xtask schema` (fluent-bit version: 3.1.5)
/// Don't modify this file manually.

#[rustfmt::skip::macros(add_snippet)]
pub static FLB_DATA: Lazy<FlbData> = Lazy::new(|| {
    let mut data = FlbData::new();

    //// Customs
    add_snippet!(data, FlbSectionType::Custom, "Calyptia", "calyptia", "custom/calyptia", [
        ("api_key", None, r#"Calyptia Cloud API Key."#),
        ("store_path", None, r#""#),
        ("calyptia_host", Some(r#"cloud-api.calyptia.com"#), r#""#),
        ("calyptia_port", Some(r#"443"#), r#""#),
        ("calyptia_tls", Some(r#"true"#), r#""#),
        ("calyptia_tls.verify", Some(r#"true"#), r#""#),
        ("add_label", None, r#"Label to append to the generated metric."#),
        ("machine_id", None, r#"Custom machine_id to be used when registering agent"#),
        ("fleet_id", None, r#"Fleet id to be used when registering agent in a fleet"#),
        ("fleet.config_dir", None, r#"Base path for the configuration directory."#),
        ("fleet.interval_sec", Some(r#"-1"#), r#"Set the collector interval"#),
        ("fleet.interval_nsec", Some(r#"-1"#), r#"Set the collector interval (nanoseconds)"#),
        ("fleet_name", None, r#"Fleet name to be used when registering agent in a fleet"#),
        ("pipeline_id", None, r#"Pipeline ID for reporting to calyptia cloud."#),
    ]);

    //// Input
    add_snippet!(data, FlbSectionType::Input, "CPU Log Based Metrics", "cpu", "input/cpu-metrics", [
        ("pid", Some(r#"-1"#), r#"Configure a single process to measure usage via their PID"#),
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (sub seconds)"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Memory Metrics", "mem", "input/memory-metrics", [
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (subseconds)"#),
        ("pid", Some(r#"0"#), r#"Set the PID of the process to measure"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Thermal", "thermal", "input/thermal", [
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
        ("name_regex", None, r#"Set thermal name regular expression filter"#),
        ("type_regex", None, r#"Set thermal type regular expression filter"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Kernel Logs", "kmsg", "input/kernel-logs", [
        ("prio_level", Some(r#"8"#), r#"The log level to filter. The kernel log is dropped if its priority is more than prio_level. Allowed values are 0-8. Default is 8."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Process Log Based Metrics", "proc", "input/process", [
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
        ("alert", Some(r#"false"#), r#"Only generate alerts if process is down"#),
        ("mem", Some(r#"true"#), r#"Append memory usage to record"#),
        ("fd", Some(r#"true"#), r#"Append fd count to record"#),
        ("proc_name", None, r#"Define process name to health check"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Disk I/O Log Based Metrics", "disk", "input/disk-io-metrics", [
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
        ("dev_name", None, r#"Set the device name"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Systemd", "systemd", "input/systemd", [
        ("path", None, r#"Set the systemd journal path"#),
        ("max_fields", Some(r#"8000"#), r#"Set the maximum fields per notification"#),
        ("max_entries", Some(r#"5000"#), r#"Set the maximum entries per notification"#),
        ("systemd_filter_type", None, r#"Set the systemd filter type to either 'and' or 'or'"#),
        ("systemd_filter", None, r#"Add a systemd filter, can be set multiple times"#),
        ("read_from_tail", Some(r#"false"#), r#"Read the journal from the end (tail)"#),
        ("lowercase", Some(r#"false"#), r#"Lowercase the fields"#),
        ("strip_underscores", Some(r#"false"#), r#"Strip undersecores from fields"#),
        ("db.sync", None, r#"Set the database sync mode: extra, full, normal or off"#),
        ("db", None, r#"Set the database path"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Network I/O Log Based Metrics", "netif", "input/network-io-metrics", [
        ("interface", None, r#"Set the interface, eg: eth0 or enp1s0"#),
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
        ("verbose", Some(r#"false"#), r#"Enable verbosity"#),
        ("test_at_init", Some(r#"false"#), r#"Testing interface at initialization"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Docker Log Based Metrics", "docker", "input/docker-metrics", [
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
        ("include", None, r#"A space-separated list of containers to include"#),
        ("exclude", None, r#"A space-separated list of containers to exclude"#),
        ("path.sysfs", Some(r#"/sys/fs/cgroup"#), r#"sysfs mount point"#),
        ("path.containers", Some(r#"/var/lib/docker/containers"#), r#"containers directory"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Docker Events", "docker_events", "input/docker-events", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("unix_path", Some(r#"/var/run/docker.sock"#), r#"Define Docker unix socket path to read events"#),
        ("buffer_size", Some(r#"8k"#), r#"Set buffer size to read events"#),
        ("parser", None, r#"Optional parser for records, if not set, records are packages under 'key'"#),
        ("key", Some(r#"message"#), r#"Set the key name to store unparsed Docker events"#),
        ("reconnect.retry_limits", Some(r#"5"#), r#"Maximum number to retry to connect docker socket"#),
        ("reconnect.retry_interval", Some(r#"1"#), r#"Retry interval to connect docker socket"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Podman Metrics", "podman_metrics", "input/podman-metrics", [
        ("scrape_interval", Some(r#"30"#), r#"Scrape interval to collect the metrics of podman containers(defaults to 30s)"#),
        ("scrape_on_start", Some(r#"false"#), r#"Scrape metrics upon start, useful to avoid waiting for 'scrape_interval' for the first round of metrics."#),
        ("path.config", None, r#"Path to podman config file"#),
        ("path.sysfs", Some(r#"/sys/fs/cgroup"#), r#"Path to sysfs subsystem directory"#),
        ("path.procfs", Some(r#"/proc"#), r#"Path to proc subsystem directory"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Process Exporter Metrics", "process_exporter_metrics", "input/process-exporter-metrics", [
        ("scrape_interval", Some(r#"5"#), r#"scrape interval to collect metrics from the node."#),
        ("path.procfs", Some(r#"/proc"#), r#"procfs mount point"#),
        ("process_include_pattern", Some(r#".+"#), r#"include list regular expression"#),
        ("process_exclude_pattern", None, r#"exclude list regular expression"#),
        ("metrics", Some(r#"cpu,io,memory,state,context_switches,fd,start_time,thread_wchan,thread"#), r#"Comma separated list of keys to enable metrics."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Node Exporter Metrics", "node_exporter_metrics", "input/node-exporter-metrics", [
        ("scrape_interval", Some(r#"5"#), r#"scrape interval to collect metrics from the node."#),
        ("collector.cpu.scrape_interval", Some(r#"0"#), r#"scrape interval to collect cpu metrics from the node."#),
        ("collector.cpufreq.scrape_interval", Some(r#"0"#), r#"scrape interval to collect cpufreq metrics from the node."#),
        ("collector.meminfo.scrape_interval", Some(r#"0"#), r#"scrape interval to collect meminfo metrics from the node."#),
        ("collector.diskstats.scrape_interval", Some(r#"0"#), r#"scrape interval to collect diskstats metrics from the node."#),
        ("collector.filesystem.scrape_interval", Some(r#"0"#), r#"scrape interval to collect filesystem metrics from the node."#),
        ("collector.uname.scrape_interval", Some(r#"0"#), r#"scrape interval to collect uname metrics from the node."#),
        ("collector.stat.scrape_interval", Some(r#"0"#), r#"scrape interval to collect stat metrics from the node."#),
        ("collector.time.scrape_interval", Some(r#"0"#), r#"scrape interval to collect time metrics from the node."#),
        ("collector.loadavg.scrape_interval", Some(r#"0"#), r#"scrape interval to collect loadavg metrics from the node."#),
        ("collector.vmstat.scrape_interval", Some(r#"0"#), r#"scrape interval to collect vmstat metrics from the node."#),
        ("collector.netdev.scrape_interval", Some(r#"0"#), r#"scrape interval to collect netdev metrics from the node."#),
        ("collector.filefd.scrape_interval", Some(r#"0"#), r#"scrape interval to collect filefd metrics from the node."#),
        ("collector.textfile.scrape_interval", Some(r#"0"#), r#"scrape interval to collect textfile metrics from the node."#),
        ("collector.systemd.scrape_interval", Some(r#"0"#), r#"scrape interval to collect systemd metrics from the node."#),
        ("collector.processes.scrape_interval", Some(r#"0"#), r#"scrape interval to collect processes metrics from the node."#),
        ("collector.thermalzone.scrape_interval", Some(r#"0"#), r#"scrape interval to collect thermal zone metrics from the node."#),
        ("collector.nvme.scrape_interval", Some(r#"0"#), r#"scrape interval to collect nvme metrics from the node."#),
        ("metrics", Some(r#"cpu,cpufreq,meminfo,diskstats,filesystem,uname,stat,time,loadavg,vmstat,netdev,filefd,systemd,nvme,thermal_zone"#), r#"Comma separated list of keys to enable metrics."#),
        ("collector.textfile.path", None, r#"Specify file path or directory to collect textfile metrics from the node."#),
        ("path.procfs", Some(r#"/proc"#), r#"procfs mount point"#),
        ("path.sysfs", Some(r#"/sys"#), r#"sysfs mount point"#),
        ("systemd_service_restart_metrics", Some(r#"false"#), r#"include systemd service restart metrics"#),
        ("systemd_unit_start_time_metrics", Some(r#"false"#), r#"include systemd unit start time metrics"#),
        ("systemd_include_service_task_metrics", Some(r#"false"#), r#"include systemd service task metrics"#),
        ("systemd_include_pattern", None, r#"include list regular expression"#),
        ("systemd_exclude_pattern", Some(r#".+\.(automount|device|mount|scope|slice)"#), r#"exclude list regular expression"#),
        ("filesystem.ignore_mount_point_regex", Some(r#"^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+|var/lib/containers/storage/.+)($|/)"#), r#"ignore regular expression for mount points"#),
        ("filesystem.ignore_filesystem_type_regex", Some(r#"^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$"#), r#"ignore regular expression for filesystem types"#),
        ("diskstats.ignore_device_regex", Some(r#"^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\d+n\d+p)\d+$"#), r#"ignore regular expression for disk devices"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Kubernetes Events", "kubernetes_events", "input/kubernetes-events", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("kube_url", Some(r#"https://kubernetes.default.svc"#), r#"Kubernetes API server URL"#),
        ("interval_sec", Some(r#"0"#), r#"Set the polling interval for each channel"#),
        ("interval_nsec", Some(r#"500000000"#), r#"Set the polling interval for each channel (sub seconds)"#),
        ("tls.debug", Some(r#"0"#), r#"set TLS debug level: 0 (no debug), 1 (error), 2 (state change), 3 (info) and 4 (verbose)"#),
        ("tls.verify", Some(r#"true"#), r#"enable or disable verification of TLS peer certificate"#),
        ("tls.vhost", None, r#"set optional TLS virtual host"#),
        ("kube_ca_file", Some(r#"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"#), r#"Kubernetes TLS CA file"#),
        ("kube_ca_path", None, r#"Kubernetes TLS ca path"#),
        ("kube_token_file", Some(r#"/var/run/secrets/kubernetes.io/serviceaccount/token"#), r#"Kubernetes authorization token file"#),
        ("kube_token_ttl", Some(r#"10m"#), r#"kubernetes token ttl, until it is reread from the token file. Default: 10m"#),
        ("kube_request_limit", Some(r#"0"#), r#"kubernetes limit parameter for events query, no limit applied when set to 0"#),
        ("kube_retention_time", Some(r#"1h"#), r#"kubernetes retention time for events. Default: 1h"#),
        ("kube_namespace", None, r#"kubernetes namespace to get events from, gets event from all namespaces by default."#),
        ("db", None, r#"set a database file to keep track of recorded kubernetes events."#),
        ("db.sync", Some(r#"normal"#), r#"set a database sync method. values: extra, full, normal and off."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Kafka", "kafka", "input/kafka", [
        ("poll_ms", Some(r#"500"#), r#"Interval in milliseconds to check for new messages."#),
        ("topics", None, r#"Set the kafka topics, delimited by commas."#),
        ("format", Some(r#"none"#), r#"Set the data format which will be used for parsing records."#),
        ("brokers", None, r#"Set the kafka brokers, delimited by commas."#),
        ("client_id", None, r#"Set the kafka client_id."#),
        ("group_id", None, r#"Set the kafka group_id."#),
        ("rdkafka.", None, r#"Set the librdkafka options"#),
        ("buffer_max_size", Some(r#"4M"#), r#"Set the maximum size of chunk"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Fluent Bit Metrics", "fluentbit_metrics", "input/fluentbit-metrics", [
        ("scrape_interval", Some(r#"2"#), r#"scrape interval to collect the internal metrics of Fluent Bit."#),
        ("scrape_on_start", Some(r#"false"#), r#"scrape metrics upon start, useful to avoid waiting for 'scrape_interval' for the first round of metrics."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Prometheus Scrape Metrics", "prometheus_scrape", "input/prometheus-scrape-metrics", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("scrape_interval", Some(r#"10s"#), r#"Scraping interval."#),
        ("buffer_max_size", Some(r#"10M"#), r#""#),
        ("metrics_path", Some(r#"/metrics"#), r#"Set the metrics URI endpoint, it must start with a forward slash."#),
        ("http_user", None, r#"Set HTTP auth user"#),
        ("http_passwd", Some(r#""#), r#"Set HTTP auth password"#),
        ("bearer_token", None, r#"Set bearer token auth"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Tail", "tail", "input/tail", [
        ("path", None, r#"pattern specifying log files or multiple ones through the use of common wildcards."#),
        ("exclude_path", None, r#"Set one or multiple shell patterns separated by commas to exclude files matching a certain criteria, e.g: 'exclude_path *.gz,*.zip'"#),
        ("key", Some(r#"log"#), r#"when a message is unstructured (no parser applied), it's appended as a string under the key name log. This option allows to define an alternative name for that key."#),
        ("read_from_head", Some(r#"false"#), r#"For new discovered files on start (without a database offset/position), read the content from the head of the file, not tail."#),
        ("refresh_interval", Some(r#"60"#), r#"interval to refresh the list of watched files expressed in seconds."#),
        ("watcher_interval", Some(r#"2s"#), r#""#),
        ("progress_check_interval", Some(r#"2s"#), r#""#),
        ("progress_check_interval_nsec", Some(r#"0"#), r#""#),
        ("rotate_wait", Some(r#"5"#), r#"specify the number of extra time in seconds to monitor a file once is rotated in case some pending data is flushed."#),
        ("docker_mode", Some(r#"false"#), r#"If enabled, the plugin will recombine split Docker log lines before passing them to any parser as configured above. This mode cannot be used at the same time as Multiline."#),
        ("docker_mode_flush", Some(r#"4"#), r#"wait period time in seconds to flush queued unfinished split lines."#),
        ("docker_mode_parser", None, r#"specify the parser name to fetch log first line for muliline log"#),
        ("path_key", None, r#"set the 'key' name where the name of monitored file will be appended."#),
        ("offset_key", None, r#"set the 'key' name where the offset of monitored file will be appended."#),
        ("ignore_older", Some(r#"0"#), r#"ignore records older than 'ignore_older'. Supports m,h,d (minutes, hours, days) syntax. Default behavior is to read all records. Option only available when a Parser is specified and it can parse the time of a record."#),
        ("buffer_chunk_size", Some(r#"32768"#), r#"set the initial buffer size to read data from files. This value is used too to increase buffer size."#),
        ("buffer_max_size", Some(r#"32768"#), r#"set the limit of the buffer size per monitored file. When a buffer needs to be increased (e.g: very long lines), this value is used to restrict how much the memory buffer can grow. If reading a file exceed this limit, the file is removed from the monitored file list."#),
        ("static_batch_size", Some(r#"50M"#), r#"On start, Fluent Bit might process files which already contains data, these files are called 'static' files. The configuration property in question set's the maximum number of bytes to process per iteration for the static files monitored."#),
        ("event_batch_size", Some(r#"50M"#), r#"When Fluent Bit is processing files in event based mode the amount ofdata available for consumption could be too much and cause the input plugin to over extend and smother other pluginsThe configuration property sets the maximum number of bytes to process per iteration for the files monitored (in event mode)."#),
        ("skip_long_lines", Some(r#"false"#), r#"if a monitored file reach it buffer capacity due to a very long line (buffer_max_size), the default behavior is to stop monitoring that file. This option alter that behavior and instruct Fluent Bit to skip long lines and continue processing other lines that fits into the buffer."#),
        ("exit_on_eof", Some(r#"false"#), r#"exit Fluent Bit when reaching EOF on a monitored file."#),
        ("skip_empty_lines", Some(r#"false"#), r#"Allows to skip empty lines."#),
        ("file_cache_advise", Some(r#"true"#), r#"Use posix_fadvise for file access. Advise not to use kernel file cache."#),
        ("inotify_watcher", Some(r#"true"#), r#"set to false to use file stat watcher instead of inotify."#),
        ("parser", None, r#"specify the parser name to process an unstructured message."#),
        ("tag_regex", None, r#"set a regex to extract fields from the file name and use them later to compose the Tag."#),
        ("db", None, r#"set a database file to keep track of monitored files and it offsets."#),
        ("db.sync", Some(r#"normal"#), r#"set a database sync method. values: extra, full, normal and off."#),
        ("db.locking", Some(r#"false"#), r#"set exclusive locking mode, increase performance but don't allow external connections to the database file."#),
        ("db.journal_mode", Some(r#"WAL"#), r#"Option to provide WAL configuration for Work Ahead Logging mechanism (WAL). Enabling WAL provides higher performance. Note that WAL is not compatible with shared network file systems."#),
        ("db.compare_filename", Some(r#"false"#), r#"This option determines whether to check both the inode and the filename when retrieving file information from the db.'true' verifies both the inode and filename, while 'false' checks only the inode (default)."#),
        ("multiline", Some(r#"false"#), r#"if enabled, the plugin will try to discover multiline messages and use the proper parsers to compose the outgoing messages. Note that when this option is enabled the Parser option is not used."#),
        ("multiline_flush", Some(r#"4"#), r#"wait period time in seconds to process queued multiline messages."#),
        ("parser_firstline", None, r#"name of the parser that matches the beginning of a multiline message. Note that the regular expression defined in the parser must include a group name (named capture)."#),
        ("parser_", None, r#"optional extra parser to interpret and structure multiline entries. This option can be used to define multiple parsers, e.g: Parser_1 ab1, Parser_2 ab2, Parser_N abN."#),
        ("multiline.parser", None, r#"specify one or multiple multiline parsers: docker, cri, go, java, etc."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Dummy", "dummy", "input/dummy", [
        ("samples", Some(r#"0"#), r#"set a number of times to generate event."#),
        ("dummy", Some(r#"{"message":"dummy"}"#), r#"set the sample record to be generated. It should be a JSON object."#),
        ("metadata", Some(r#"{}"#), r#"set the sample metadata to be generated. It should be a JSON object."#),
        ("rate", Some(r#"1"#), r#"set a number of events per second."#),
        ("interval_sec", Some(r#"0"#), r#"set seconds of interval to generate events. overrides rate setting."#),
        ("interval_nsec", Some(r#"0"#), r#"set nanoseconds of interval to generate events. overrides rate setting."#),
        ("copies", Some(r#"1"#), r#"set the number of copies to generate per collectd."#),
        ("start_time_sec", Some(r#"-1"#), r#"set a dummy base timestamp in seconds."#),
        ("start_time_nsec", Some(r#"-1"#), r#"set a dummy base timestamp in nanoseconds."#),
        ("fixed_timestamp", Some(r#"off"#), r#"used a fixed timestamp, allows the message to pre-generated once."#),
        ("flush_on_startup", Some(r#"false"#), r#"generate the first event on startup"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Head", "head", "input/head", [
        ("file", None, r#"Set the file"#),
        ("key", Some(r#"head"#), r#"Set the record key"#),
        ("buf_size", Some(r#"256"#), r#"Set the read buffer size"#),
        ("split_line", Some(r#"false"#), r#"generate key/value pair per line"#),
        ("lines", Some(r#"0"#), r#"Line number to read"#),
        ("add_path", Some(r#"false"#), r#"append filepath to records"#),
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Health", "health", "input/health", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("alert", Some(r#"false"#), r#"Only generate records when the port is down"#),
        ("add_host", Some(r#"false"#), r#"Append hostname to each record"#),
        ("add_port", Some(r#"false"#), r#"Append port to each record"#),
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "HTTP", "http", "input/http", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("http2", Some(r#"true"#), r#""#),
        ("buffer_max_size", Some(r#"4M"#), r#""#),
        ("buffer_chunk_size", Some(r#"512K"#), r#""#),
        ("success_header", None, r#"Add an HTTP header key/value pair on success. Multiple headers can be set"#),
        ("tag_key", None, r#""#),
        ("successful_response_code", Some(r#"201"#), r#"Set successful response code. 200, 201 and 204 are supported."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Collectd", "collectd", "input/collectd", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("typesdb", Some(r#"/usr/share/collectd/types.db"#), r#"Set the types database filename"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "StatsD", "statsd", "input/statsd", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "OpenTelemetry", "opentelemetry", "input/opentelemetry", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("http2", Some(r#"true"#), r#""#),
        ("buffer_max_size", Some(r#"4M"#), r#""#),
        ("buffer_chunk_size", Some(r#"512K"#), r#""#),
        ("tag_key", None, r#""#),
        ("tag_from_uri", Some(r#"true"#), r#"If true, tag will be created from uri. e.g. v1_metrics from /v1/metrics ."#),
        ("successful_response_code", Some(r#"201"#), r#"Set successful response code. 200, 201 and 204 are supported."#),
        ("raw_traces", Some(r#"false"#), r#"Forward traces without processing"#),
        ("logs_metadata_key", Some(r#"otlp"#), r#""#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Elasticsearch", "elasticsearch", "input/elasticsearch", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("http2", Some(r#"true"#), r#""#),
        ("buffer_max_size", Some(r#"4M"#), r#"Set the maximum size of buffer"#),
        ("buffer_chunk_size", Some(r#"512K"#), r#"Set the buffer chunk size"#),
        ("tag_key", None, r#"Specify a key name for extracting as a tag"#),
        ("meta_key", Some(r#"@meta"#), r#"Specify a key name for meta information"#),
        ("hostname", Some(r#"localhost"#), r#"Specify hostname or FQDN. This parameter is effective for sniffering node information."#),
        ("version", Some(r#"8.0.0"#), r#"Specify returning Elasticsearch server version."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Splunk", "splunk", "input/splunk", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("http2", Some(r#"true"#), r#""#),
        ("buffer_max_size", Some(r#"4M"#), r#""#),
        ("buffer_chunk_size", Some(r#"512K"#), r#""#),
        ("success_header", None, r#"Add an HTTP header key/value pair on success. Multiple headers can be set"#),
        ("splunk_token", None, r#"Set valid Splunk HEC tokens for the requests"#),
        ("store_token_in_metadata", Some(r#"true"#), r#"Store Splunk HEC tokens in matadata. If set as false, they will be stored into records."#),
        ("splunk_token_key", Some(r#"@splunk_token"#), r#"Set a record key for storing Splunk HEC token for the request"#),
        ("tag_key", None, r#""#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Prometheus Remote Write", "prometheus_remote_write", "input/prometheus-remote-write", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("http2", Some(r#"true"#), r#""#),
        ("buffer_max_size", Some(r#"4M"#), r#""#),
        ("buffer_chunk_size", Some(r#"512K"#), r#""#),
        ("uri", None, r#"Specify an optional HTTP URI for the target web server, e.g: /something"#),
        ("tag_from_uri", Some(r#"true"#), r#"If true, tag will be created from uri. e.g. v1_metrics from /v1/metrics ."#),
        ("successful_response_code", Some(r#"201"#), r#"Set successful response code. 200, 201 and 204 are supported."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "NGINX Exporter Metrics", "nginx_metrics", "input/nginx", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("status_url", Some(r#"/status"#), r#"Define URL of stub status handler"#),
        ("nginx_plus", Some(r#"true"#), r#"Turn on NGINX plus mode"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Serial Interface", "serial", "input/serial-interface", [
        ("file", None, r#"Set the serial character device file name"#),
        ("bitrate", None, r#"Set the serial bitrate (baudrate)"#),
        ("separator", None, r#"Set the record separator"#),
        ("format", None, r#"Set the serial format: json or none"#),
        ("min_bytes", Some(r#"0"#), r#"Set the serial minimum bytes"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Standard Input", "stdin", "input/standard-input", [
        ("parser", None, r#"Set and use a fluent-bit parser"#),
        ("buffer_size", None, r#"Set the read buffer size"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Syslog", "syslog", "input/syslog", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("mode", None, r#"Set the socket mode: unix_tcp, unix_udp, tcp or udp"#),
        ("path", None, r#"Set the path for the UNIX socket"#),
        ("unix_perm", None, r#"Set the permissions for the UNIX socket"#),
        ("buffer_chunk_size", Some(r#"32768"#), r#"Set the buffer chunk size"#),
        ("buffer_max_size", None, r#"Set the buffer chunk size"#),
        ("parser", None, r#"Set the parser"#),
        ("receive_buffer_size", None, r#"Set the socket receiving buffer size"#),
        ("raw_message_key", None, r#"Key where the raw message will be preserved"#),
        ("source_address_key", None, r#"Key where the source address will be injected"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Exec Wasi", "exec_wasi", "input/exec-wasi", [
        ("wasi_path", None, r#"Set the path of WASM program to execute"#),
        ("accessible_paths", Some(r#"."#), r#"Specifying paths to be accessible from a WASM program.Default value is current working directory"#),
        ("parser", None, r#"Set a parser"#),
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (nanoseconds)"#),
        ("buf_size", Some(r#"4096"#), r#"Set the buffer size"#),
        ("bool", Some(r#"false"#), r#"execute the command only once"#),
        ("wasm_heap_size", Some(r#"8192"#), r#"Set the heap size of wasm runtime"#),
        ("wasm_stack_size", Some(r#"8192"#), r#"Set the stack size of wasm runtime"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "TCP", "tcp", "input/tcp", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("format", None, r#"Set the format: json or none"#),
        ("separator", None, r#"Set separator"#),
        ("chunk_size", None, r#"Set the chunk size"#),
        ("buffer_size", None, r#"Set the buffer size"#),
        ("source_address_key", None, r#"Key where the source address will be injected"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "MQTT", "mqtt", "input/mqtt", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("payload_key", None, r#"Key where the payload will be preserved"#),
        ("buffer_size", Some(r#"2048"#), r#"Maximum payload size"#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Forward", "forward", "input/forward", [
        ("listen", Some(r#"0.0.0.0"#), r#"Listen Address"#),
        ("host", Some(r#"localhost"#), r#"Hostname"#),
        ("port", Some(r#"0"#), r#"Listen Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("tag_prefix", None, r#"Prefix incoming tag with the defined value."#),
        ("shared_key", None, r#"Shared key for authentication"#),
        ("self_hostname", None, r#"Hostname"#),
        ("security.users", None, r#"Specify username and password pairs."#),
        ("unix_path", None, r#"The path to unix socket to receive a Forward message."#),
        ("unix_perm", None, r#"Set the permissions for the UNIX socket"#),
        ("buffer_chunk_size", Some(r#"1024000"#), r#"The buffer memory size used to receive a Forward message."#),
        ("buffer_max_size", Some(r#"6144000"#), r#"The maximum buffer memory size used to receive a Forward message."#),
    ]);
    add_snippet!(data, FlbSectionType::Input, "Random", "random", "input/random", [
        ("samples", Some(r#"-1"#), r#"Number of samples to send, -1 for infinite"#),
        ("interval_sec", Some(r#"1"#), r#"Set the collector interval"#),
        ("interval_nsec", Some(r#"0"#), r#"Set the collector interval (sub seconds)"#),
    ]);

    //// Filter
    add_snippet!(data, FlbSectionType::Filter, "AWS Metadata", "aws", "filter/aws-metadata", [
        ("imds_version", Some(r#"v2"#), r#"Specifies which version of the EC2 instance metadata service will be used: 'v1' or 'v2'. 'v2' may not work if you run Fluent Bit in a container."#),
        ("az", Some(r#"true"#), r#"Enable EC2 instance availability zone"#),
        ("ec2_instance_id", Some(r#"true"#), r#"Enable EC2 instance ID"#),
        ("ec2_instance_type", Some(r#"false"#), r#"Enable EC2 instance type"#),
        ("private_ip", Some(r#"false"#), r#"Enable EC2 instance private IP"#),
        ("vpc_id", Some(r#"false"#), r#"Enable EC2 instance VPC ID"#),
        ("ami_id", Some(r#"false"#), r#"Enable EC2 instance Image ID"#),
        ("account_id", Some(r#"false"#), r#"Enable EC2 instance Account ID"#),
        ("hostname", Some(r#"false"#), r#"Enable EC2 instance hostname"#),
        ("tags_enabled", Some(r#"false"#), r#"Enable EC2 instance tags, injects all tags if tags_include and tags_exclude are empty"#),
        ("tags_include", Some(r#""#), r#"Defines list of specific EC2 tag keys to inject into the logs; tag keys must be separated by "," character; tags which are not present in this list will be ignored; e.g.: "Name,tag1,tag2""#),
        ("tags_exclude", Some(r#""#), r#"Defines list of specific EC2 tag keys not to inject into the logs; tag keys must be separated by "," character; if both tags_include and tags_exclude are specified, configuration is invalid and plugin fails"#),
        ("retry_interval_s", Some(r#"300"#), r#"Defines minimum duration between retries for fetching metadata groups"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "CheckList", "checklist", "filter/checklist", [
        ("file", None, r#"Specify the file that contains the patterns to lookup."#),
        ("mode", Some(r#"exact"#), r#"Set the check mode: 'exact' or 'partial'."#),
        ("print_query_time", Some(r#"false"#), r#"Print to stdout the elapseed query time for every matched record"#),
        ("ignore_case", Some(r#"false"#), r#"Compare strings by ignoring case."#),
        ("lookup_key", Some(r#"log"#), r#"Name of the key to lookup."#),
        ("record", None, r#"Name of record key to add and its value, it accept two values,e.g 'record mykey my val'. You can add many 'record' entries as needed."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "ECS Metadata", "ecs", "filter/ecs-metadata", [
        ("add", None, r#"Add a metadata key/value pair with the given key and given value from the given template. Format is `Add KEY TEMPLATE`."#),
        ("ecs_tag_prefix", Some(r#""#), r#"This filter must obtain the 12 character container short ID to query for ECS Task metadata. The filter removes the prefx from the tag and then assumes the next 12 characters are the short container ID. If the container short ID, is not found in the tag, the filter can/must fallback to only attaching cluster metadata (cluster name, container instance ID/ARN, and ECS Agent version)."#),
        ("cluster_metadata_only", Some(r#"false"#), r#"Only attempt to attach the cluster related metadata to logs (cluster name, container instance ID/ARN, and ECS Agent version). With this option off, if this filter can not obtain the task metadata for a log, it will output errors. Use this option if you have logs that are not part of an ECS task (ex: Docker Daemon logs)."#),
        ("ecs_meta_cache_ttl", Some(r#"3600"#), r#"Configurable TTL for cached ECS Task Metadata. Default 3600s (1 hour)For example, set this value to 600 or 600s or 10m and cache entries which have been created more than 10 minutes will be evicted.Cache eviction is needed to purge task metadata for tasks that have been stopped."#),
        ("ecs_meta_host", Some(r#"127.0.0.1"#), r#"The host name at which the ECS Agent Introspection endpoint is reachable. Defaults to 127.0.0.1"#),
        ("ecs_meta_port", Some(r#"51678"#), r#"The port at which the ECS Agent Introspection endpoint is reachable. Defaults to 51678"#),
        ("agent_endpoint_retries", Some(r#"2"#), r#"Number of retries for failed metadata requests to ECS Agent Introspection endpoint. The most common cause of failed metadata requests is that the container the metadata request was made for is not part of an ECS Task. Check if you have non-task containers and docker dual logging enabled."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Record Modifier", "record_modifier", "filter/record-modifier", [
        ("record", None, r#"Append fields. This parameter needs key and value pair."#),
        ("remove_key", None, r#"If the key is matched, that field is removed."#),
        ("allowlist_key", None, r#"If the key is not matched, that field is removed."#),
        ("whitelist_key", None, r#"(Alias of allowlist_key)"#),
        ("uuid_key", None, r#"If set, the plugin generates uuid per record."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Sysinfo", "sysinfo", "filter/sysinfo", [
        ("fluentbit_version_key", None, r#"Specify the key name for fluent-bit version."#),
        ("os_name_key", None, r#"Specify the key name for os name. e.g. linux, win64 or macos."#),
        ("hostname_key", None, r#"Specify the key name for hostname."#),
        ("os_version_key", None, r#"Specify the key name for os version. It is not supported on some platforms."#),
        ("kernel_version_key", None, r#"Specify the key name for kernel version. It is not supported on some platforms."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Throttle", "throttle", "filter/throttle", [
        ("rate", Some(r#"1"#), r#"Set throttle rate"#),
        ("window", Some(r#"5"#), r#"Set throttle window"#),
        ("print_status", Some(r#"false"#), r#"Set whether or not to print status information"#),
        ("interval", Some(r#"1"#), r#"Set the slide interval"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Type Converter", "type_converter", "filter/type-converter", [
        ("int_key", None, r#"Convert integer to other type. e.g. int_key id id_str string"#),
        ("uint_key", None, r#"Convert unsinged integer to other type. e.g. uint_key id id_str string"#),
        ("float_key", None, r#"Convert float to other type. e.g. float_key ratio id_str string"#),
        ("str_key", None, r#"Convert string to other type. e.g. str_key id id_val integer"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Kubernetes", "kubernetes", "filter/kubernetes", [
        ("buffer_size", Some(r#"32K"#), r#"buffer size to receive response from API server"#),
        ("tls.debug", Some(r#"0"#), r#"set TLS debug level: 0 (no debug), 1 (error), 2 (state change), 3 (info) and 4 (verbose)"#),
        ("tls.verify", Some(r#"true"#), r#"enable or disable verification of TLS peer certificate"#),
        ("tls.vhost", None, r#"set optional TLS virtual host"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"enable or disable to verify hostname"#),
        ("merge_log", Some(r#"false"#), r#"merge 'log' key content as individual keys"#),
        ("merge_parser", None, r#"specify a 'parser' name to parse the 'log' key content"#),
        ("merge_log_key", None, r#"set the 'key' name where the content of 'key' will be placed. Only used if the option 'merge_log' is enabled"#),
        ("merge_log_trim", Some(r#"true"#), r#"remove ending '\n' or '\r' characters from the log content"#),
        ("keep_log", Some(r#"true"#), r#"keep original log content if it was successfully parsed and merged"#),
        ("kube_url", Some(r#"https://kubernetes.default.svc"#), r#"Kubernetes API server URL"#),
        ("kube_meta_preload_cache_dir", None, r#"set directory with metadata files"#),
        ("kube_ca_file", Some(r#"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"#), r#"Kubernetes TLS CA file"#),
        ("kube_ca_path", None, r#"Kubernetes TLS ca path"#),
        ("kube_tag_prefix", Some(r#"kube.var.log.containers."#), r#"prefix used in tag by the input plugin"#),
        ("kube_token_file", Some(r#"/var/run/secrets/kubernetes.io/serviceaccount/token"#), r#"Kubernetes authorization token file"#),
        ("kube_token_command", None, r#"command to get Kubernetes authorization token"#),
        ("labels", Some(r#"true"#), r#"include Kubernetes labels on every record"#),
        ("annotations", Some(r#"true"#), r#"include Kubernetes annotations on every record"#),
        ("namespace_labels", Some(r#"false"#), r#"include Kubernetes namespace labels on every record"#),
        ("namespace_annotations", Some(r#"false"#), r#"include Kubernetes namespace annotations on every record"#),
        ("namespace_metadata_only", Some(r#"false"#), r#"ignore pod metadata entirely and only fetch namespace metadata"#),
        ("k8s-logging.parser", Some(r#"false"#), r#"allow Pods to suggest a parser"#),
        ("k8s-logging.exclude", Some(r#"false"#), r#"allow Pods to exclude themselves from the logging pipeline"#),
        ("use_journal", Some(r#"false"#), r#"use Journald (Systemd) mode"#),
        ("regex_parser", None, r#"optional regex parser to extract metadata from container name or container log file name"#),
        ("dummy_meta", Some(r#"false"#), r#"use 'dummy' metadata, do not talk to API server"#),
        ("dns_retries", Some(r#"6"#), r#"dns lookup retries N times until the network start working"#),
        ("dns_wait_time", Some(r#"30"#), r#"dns interval between network status checks"#),
        ("cache_use_docker_id", Some(r#"false"#), r#"fetch K8s meta when docker_id is changed"#),
        ("use_tag_for_meta", Some(r#"false"#), r#"use tag associated to retrieve metadata instead of kube-server"#),
        ("use_kubelet", Some(r#"false"#), r#"use kubelet to get metadata instead of kube-server"#),
        ("kubelet_host", Some(r#"127.0.0.1"#), r#"kubelet host to connect with when using kubelet"#),
        ("kubelet_port", Some(r#"10250"#), r#"kubelet port to connect with when using kubelet"#),
        ("kube_token_ttl", Some(r#"10m"#), r#"kubernetes token ttl, until it is reread from the token file. Default: 10m"#),
        ("kube_meta_cache_ttl", Some(r#"0"#), r#"configurable TTL for K8s cached metadata. By default, it is set to 0 which means TTL for cache entries is disabled and cache entries are evicted at random when capacity is reached. In order to enable this option, you should set the number to a time interval. For example, set this value to 60 or 60s and cache entries which have been created more than 60s will be evicted"#),
        ("kube_meta_namespace_cache_ttl", Some(r#"15m"#), r#"configurable TTL for K8s cached namespace metadata. By default, it is set to 15m and cached entries will be evicted after 15m.Setting this to 0 will disable the cache TTL and will evict entries once the cache reaches capacity."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Modify", "modify", "filter/modify", [
        ("Set", None, r#"Add a key/value pair with key KEY and value VALUE. If KEY already exists, this field is overwritten."#),
        ("Add", None, r#"Add a key/value pair with key KEY and value VALUE if KEY does not exist"#),
        ("Remove", None, r#"Remove a key/value pair with key KEY if it exists"#),
        ("Remove_wildcard", None, r#"Remove all key/value pairs with key matching wildcard KEY"#),
        ("Remove_regex", None, r#"Remove all key/value pairs with key matching regexp KEY"#),
        ("Move_To_Start", None, r#"Move key/value pairs with keys matching KEY to the start of the message"#),
        ("Move_To_End", None, r#"Move key/value pairs with keys matching KEY to the end of the message"#),
        ("Rename", None, r#"Rename a key/value pair with key KEY to RENAMED_KEY if KEY exists AND RENAMED_KEY does not exist"#),
        ("Hard_Rename", None, r#"Rename a key/value pair with key KEY to RENAMED_KEY if KEY exists. If RENAMED_KEY already exists, this field is overwritten"#),
        ("Copy", None, r#"Copy a key/value pair with key KEY to COPIED_KEY if KEY exists AND COPIED_KEY does not exist"#),
        ("Hard_copy", None, r#"Copy a key/value pair with key KEY to COPIED_KEY if KEY exists. If COPIED_KEY already exists, this field is overwritten"#),
        ("Condition", None, r#"Set the condition to modify. Key_exists, Key_does_not_exist, A_key_matches, No_key_matches, Key_value_equals, Key_value_does_not_equal, Key_value_matches, Key_value_does_not_match, Matching_keys_have_matching_values and Matching_keys_do_not_have_matching_values are supported."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Multiline", "multiline", "filter/multiline-stacktrace", [
        ("debug_flush", Some(r#"false"#), r#"enable debugging for concatenation flush to stdout"#),
        ("buffer", Some(r#"true"#), r#"Enable buffered mode. In buffered mode, the filter can concatenate multilines from inputs that ingest records one by one (ex: Forward), rather than in chunks, re-emitting them into the beggining of the pipeline using the in_emitter instance. With buffer off, this filter will not work with most inputs, except tail."#),
        ("mode", Some(r#"parser"#), r#"Mode can be 'parser' for regex concat, or 'partial_message' to concat split docker logs."#),
        ("flush_ms", Some(r#"2000"#), r#"Flush time for pending multiline records"#),
        ("multiline.parser", None, r#"specify one or multiple multiline parsers: docker, cri, go, java, etc."#),
        ("multiline.key_content", None, r#"specify the key name that holds the content to process."#),
        ("emitter_name", None, r#""#),
        ("emitter_storage.type", Some(r#"memory"#), r#""#),
        ("emitter_mem_buf_limit", Some(r#"10M"#), r#"set a memory buffer limit to restrict memory usage of emitter"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Nest", "nest", "filter/nest", [
        ("Operation", None, r#"Select the operation nest or lift"#),
        ("Wildcard", None, r#"Nest records which field matches the wildcard"#),
        ("Nest_under", None, r#"Nest records matching the Wildcard under this key"#),
        ("Nested_under", None, r#"Lift records nested under the Nested_under key"#),
        ("Add_prefix", None, r#"Prefix affected keys with this string"#),
        ("Remove_prefix", None, r#"Remove prefix from affected keys if it matches this string"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Parser", "parser", "filter/parser", [
        ("Key_Name", None, r#"Specify field name in record to parse."#),
        ("Parser", None, r#"Specify the parser name to interpret the field. Multiple Parser entries are allowed (one per line)."#),
        ("Preserve_Key", Some(r#"false"#), r#"Keep original Key_Name field in the parsed result. If false, the field will be removed."#),
        ("Reserve_Data", Some(r#"false"#), r#"Keep all other original fields in the parsed result. If false, all other original fields will be removed."#),
        ("Unescape_key", None, r#"(deprecated)"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Expect", "expect", "filter/expect", [
        ("key_exists", None, r#"check that the given key name exists in the record"#),
        ("key_not_exists", None, r#"check that the given key name do not exists in the record"#),
        ("key_val_is_null", None, r#"check that the value of the key is NULL"#),
        ("key_val_is_not_null", None, r#"check that the value of the key is NOT NULL"#),
        ("key_val_eq", None, r#"check that the value of the key equals the given value"#),
        ("action", Some(r#"warn"#), r#"action to take when a rule does not match: 'warn', 'exit' or 'result_key'."#),
        ("result_key", Some(r#"matched"#), r#"specify the key name to append a boolean that indicates rule is matched or not. This key is to be used only when 'action' is 'result_key'."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Grep", "grep", "filter/grep", [
        ("regex", None, r#"Keep records in which the content of KEY matches the regular expression."#),
        ("exclude", None, r#"Exclude records in which the content of KEY matches the regular expression."#),
        ("logical_op", Some(r#"legacy"#), r#"Specify whether to use logical conjuciton or disjunction. legacy, AND and OR are allowed."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Rewrite Tag", "rewrite_tag", "filter/rewrite-tag", [
        ("rule", None, r#""#),
        ("emitter_name", None, r#""#),
        ("emitter_storage.type", Some(r#"memory"#), r#""#),
        ("emitter_mem_buf_limit", Some(r#"10M"#), r#"set a memory buffer limit to restrict memory usage of emitter"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Log to Metrics", "log_to_metrics", "filter/log_to_metrics", [
        ("regex", None, r#"Optional filter for records in which the content of KEY matches the regular expression."#),
        ("exclude", None, r#"Optional filter for records in which the content of KEY does not matches the regular expression."#),
        ("metric_mode", Some(r#"counter"#), r#"Mode selector. Values counter, gauge, or histogram. Summary is not supported"#),
        ("value_field", None, r#"Numeric field to use for gauge or histogram"#),
        ("metric_name", Some(r#"a"#), r#"Name of the metric"#),
        ("metric_namespace", Some(r#"log_metric"#), r#"Namespace of the metric"#),
        ("metric_subsystem", None, r#"Subsystem of the metric"#),
        ("metric_description", None, r#"Help text for metric"#),
        ("kubernetes_mode", Some(r#"false"#), r#"Enable kubernetes log metric fields"#),
        ("add_label", None, r#"Add a label to the metric by supporting record accessor pattern"#),
        ("label_field", None, r#"Specify message field that should be included in the metric"#),
        ("bucket", None, r#"Specify bucket for histogram metric"#),
        ("tag", None, r#"Metric Tag"#),
        ("emitter_name", None, r#"Name of the emitter (advanced users)"#),
        ("emitter_mem_buf_limit", Some(r#"10M"#), r#"set a buffer limit to restrict memory usage of metrics emitter"#),
        ("discard_logs", Some(r#"false"#), r#"Flag that defines if logs should be discarded after processing. This applies for all logs, no matter if they have emitted metrics or not."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Lua", "lua", "filter/lua", [
        ("script", None, r#"The path of lua script."#),
        ("code", None, r#"String that contains the Lua script source code"#),
        ("call", None, r#"Lua function name that will be triggered to do filtering."#),
        ("type_int_key", None, r#"If these keys are matched, the fields are converted to integer. If more than one key, delimit by space."#),
        ("type_array_key", None, r#"If these keys are matched, the fields are converted to array. If more than one key, delimit by space."#),
        ("protected_mode", Some(r#"true"#), r#"If enabled, Lua script will be executed in protected mode. It prevents to crash when invalid Lua script is executed."#),
        ("time_as_table", Some(r#"false"#), r#"If enabled, Fluent-bit will pass the timestamp as a Lua table with keys "sec" for seconds since epoch and "nsec" for nanoseconds."#),
        ("enable_flb_null", Some(r#"false"#), r#"If enabled, null will be converted to flb_null in Lua. It is useful to prevent removing key/value since nil is a special value to remove key value from map in Lua."#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Standard Output", "stdout", "filter/standard-output", [
    ]);
    add_snippet!(data, FlbSectionType::Filter, "GeoIP2 Filter", "geoip2", "filter/geoip2-filter", [
        ("database", None, r#"Set the geoip2 database path"#),
        ("lookup_key", None, r#"Add a lookup_key"#),
        ("record", None, r#"Add a record to the output base on geoip2"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Nightfall", "nightfall", "filter/nightfall", [
        ("nightfall_api_key", None, r#"The Nightfall API key to scan your logs with."#),
        ("policy_id", None, r#"The Nightfall policy ID to scan your logs with."#),
        ("sampling_rate", Some(r#"1"#), r#"The sampling rate for scanning, must be (0,1]. 1 means all logs will be scanned."#),
        ("tls.debug", Some(r#"0"#), r#"Set TLS debug level: 0 (no debug), 1 (error), 2 (state change), 3 (info) and 4 (verbose)"#),
        ("tls.verify", Some(r#"true"#), r#"Enable or disable verification of TLS peer certificate"#),
        ("tls.vhost", None, r#"Set optional TLS virtual host"#),
        ("tls.ca_path", None, r#"Path to root certificates on the system"#),
    ]);
    add_snippet!(data, FlbSectionType::Filter, "Wasm", "wasm", "filter/wasm", [
        ("event_format", None, r#"Sepecify the ingesting event format for wasm program"#),
        ("wasm_path", None, r#"Set the wasm path to execute"#),
        ("accessible_paths", Some(r#"."#), r#"Specifying paths to be accessible from a WASM program.Default value is current working directory"#),
        ("function_name", None, r#"Set the function name in wasm to execute"#),
        ("wasm_heap_size", Some(r#"8192"#), r#"Set the heap size of wasm runtime"#),
        ("wasm_stack_size", Some(r#"8192"#), r#"Set the stack size of wasm runtime"#),
    ]);

    //// Output
    add_snippet!(data, FlbSectionType::Output, "Azure Log Analytics", "azure", "output/azure", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("customer_id", None, r#"Customer ID or WorkspaceID string."#),
        ("shared_key", None, r#"The primary or the secondary Connected Sources client authentication key."#),
        ("log_type", Some(r#"fluentbit"#), r#"The name of the event type."#),
        ("log_type_key", None, r#"If included, the value for this key will be looked upon in the record and if present, will over-write the `log_type`. If the key/value is not found in the record then the `log_type` option will be used. "#),
        ("time_key", Some(r#"@timestamp"#), r#"Optional parameter to specify the key name where the timestamp will be stored."#),
        ("time_generated", Some(r#"false"#), r#"If enabled, the HTTP request header 'time-generated-field' will be included so Azure can override the timestamp with the key specified by 'time_key' option."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Azure Blob", "azure_blob", "output/azure_blob", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("account_name", None, r#"Azure account name (mandatory)"#),
        ("container_name", None, r#"Container name (mandatory)"#),
        ("auto_create_container", Some(r#"true"#), r#"Auto create container if it don't exists"#),
        ("blob_type", Some(r#"appendblob"#), r#"Set the block type: appendblob or blockblob"#),
        ("compress", None, r#"Set payload compression in network transfer. Option available is 'gzip'"#),
        ("compress_blob", Some(r#"false"#), r#"Enable block blob GZIP compression in the final blob file. This option is not compatible with 'appendblob' block type"#),
        ("emulator_mode", Some(r#"false"#), r#"Use emulator mode, enable it if you want to use Azurite"#),
        ("shared_key", None, r#"Azure shared key"#),
        ("endpoint", None, r#"Custom full URL endpoint to use an emulator"#),
        ("path", None, r#"Set a path for your blob"#),
        ("date_key", Some(r#"@timestamp"#), r#"Name of the key that will have the record timestamp"#),
        ("auth_type", Some(r#"key"#), r#"Set the auth type: key or sas"#),
        ("sas_token", None, r#"Azure Blob SAS token"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Azure Logs Ingestion API", "azure_logs_ingestion", "output/azure_logs_ingestion", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tenant_id", None, r#"Set the tenant ID of the AAD application"#),
        ("client_id", None, r#"Set the client/app ID of the AAD application"#),
        ("client_secret", None, r#"Set the client secret of the AAD application"#),
        ("dce_url", None, r#"Data Collection Endpoint(DCE) URI (e.g. https://la-endpoint-q12a.eastus-1.ingest.monitor.azure.com)"#),
        ("dcr_id", None, r#"Data Collection Rule (DCR) immutable ID"#),
        ("table_name", None, r#"The name of the custom log table, including '_CL' suffix"#),
        ("time_key", Some(r#"@timestamp"#), r#"[Optional] Specify the key name where the timestamp will be stored."#),
        ("time_generated", Some(r#"false"#), r#"If enabled, will generate a timestamp and append it to JSON. The key name is set by the 'time_key' parameter"#),
        ("compress", Some(r#"false"#), r#"Enable HTTP payload compression (gzip)."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Azure Data Explorer", "azure_kusto", "output/azure_kusto", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tenant_id", None, r#"Set the tenant ID of the AAD application used for authentication"#),
        ("client_id", None, r#"Set the client ID (Application ID) of the AAD application used for authentication"#),
        ("client_secret", None, r#"Set the client secret (Application Password) of the AAD application used for authentication"#),
        ("ingestion_endpoint", None, r#"Set the Kusto cluster's ingestion endpoint URL (e.g. https://ingest-mycluster.eastus.kusto.windows.net)"#),
        ("database_name", None, r#"Set the database name"#),
        ("table_name", None, r#"Set the table name"#),
        ("ingestion_mapping_reference", None, r#"Set the ingestion mapping reference"#),
        ("log_key", Some(r#"log"#), r#"The key name of event payload"#),
        ("include_tag_key", Some(r#"true"#), r#"If enabled, tag is appended to output. The key name is used 'tag_key' property."#),
        ("tag_key", Some(r#"tag"#), r#"The key name of tag. If 'include_tag_key' is false, This property is ignored"#),
        ("include_time_key", Some(r#"true"#), r#"If enabled, time is appended to output. The key name is used 'time_key' property."#),
        ("time_key", Some(r#"timestamp"#), r#"The key name of the time. If 'include_time_key' is false, This property is ignored"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Google Cloud BigQuery", "bigquery", "output/bigquery", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("google_service_credentials", None, r#"Set the path for the google service credentials file"#),
        ("enable_identity_federation", Some(r#"false"#), r#"Enable identity federation"#),
        ("aws_region", None, r#"Enable identity federation"#),
        ("project_number", None, r#"Set project number"#),
        ("pool_id", None, r#"Set the pool id"#),
        ("provider_id", None, r#"Set the provider id"#),
        ("google_service_account", None, r#"Set the google service account"#),
        ("service_account_email", None, r#"Set the service account email"#),
        ("service_account_secret", None, r#"Set the service account secret"#),
        ("project_id", None, r#"Set the project id"#),
        ("dataset_id", None, r#"Set the dataset id"#),
        ("table_id", None, r#"Set the table id"#),
        ("skip_invalid_rows", Some(r#"false"#), r#"Enable skipping of invalid rows"#),
        ("ignore_unknown_values", Some(r#"false"#), r#"Enable ignoring unknown value"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Counter", "counter", "output/counter", [
    ]);
    add_snippet!(data, FlbSectionType::Output, "Datadog", "datadog", "output/datadog", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("compress", Some(r#"false"#), r#"compresses the payload in GZIP format, Datadog supports and recommends setting this to 'gzip'."#),
        ("apikey", None, r#"Datadog API key"#),
        ("dd_service", None, r#"The human readable name for your service generating the logs  (e.g. the name of your application or database). If unset, Datadog will look for the service using Service Remapper in Log Management (by default it will look at the `service` and `syslog.appname` attributes)."#),
        ("dd_source", None, r#"A human readable name for the underlying technology of your service (e.g. 'postgres' or 'nginx'). If unset, Datadog will expect the source to be set as the `ddsource` attribute."#),
        ("dd_tags", None, r#"The tags you want to assign to your logs in Datadog. If unset, Datadog will expect the tags in the `ddtags` attribute."#),
        ("dd_hostname", None, r#"The host that emitted logs should be associated with. If unset, Datadog will expect the host to be set as `host`, `hostname`, or `syslog.hostname` attributes. See Datadog Logs preprocessor documentation for up-to-date recognized attributes."#),
        ("proxy", None, r#"Specify an HTTP Proxy. The expected format of this value is http://host:port. Note that https is not supported yet."#),
        ("include_tag_key", Some(r#"false"#), r#"If enabled, tag is appended to output. The key name is used 'tag_key' property."#),
        ("tag_key", Some(r#"tagkey"#), r#"The key name of tag. If 'include_tag_key' is false, This property is ignored"#),
        ("dd_message_key", Some(r#"log"#), r#"By default, the plugin searches for the key 'log' and remap the value to the key 'message'. If the property is set, the plugin will search the property name key."#),
        ("provider", None, r#"To activate the remapping, specify configuration flag provider with value 'ecs'"#),
        ("json_date_key", Some(r#"timestamp"#), r#"Date key name for output."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Elasticsearch", "es", "output/elasticsearch", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("index", Some(r#"fluent-bit"#), r#"Set an index name"#),
        ("type", Some(r#"_doc"#), r#"Set the document type property"#),
        ("suppress_type_name", Some(r#"false"#), r#"If true, mapping types is removed. (for v7.0.0 or later)"#),
        ("http_user", None, r#"Optional username credential for Elastic X-Pack access"#),
        ("http_passwd", Some(r#""#), r#"Password for user defined in HTTP_User"#),
        ("compress", None, r#"Set payload compression mechanism. Option available is 'gzip'"#),
        ("cloud_id", None, r#"Elastic cloud ID of the cluster to connect to"#),
        ("cloud_auth", None, r#"Elastic cloud authentication credentials"#),
        ("aws_auth", Some(r#"false"#), r#"Enable AWS Sigv4 Authentication"#),
        ("aws_region", None, r#"AWS Region of your Amazon OpenSearch Service cluster"#),
        ("aws_sts_endpoint", None, r#"Custom endpoint for the AWS STS API, used with the AWS_Role_ARN option"#),
        ("aws_role_arn", None, r#"AWS IAM Role to assume to put records to your Amazon OpenSearch cluster"#),
        ("aws_external_id", None, r#"External ID for the AWS IAM Role specified with `aws_role_arn`"#),
        ("aws_service_name", Some(r#"es"#), r#"AWS Service Name"#),
        ("aws_profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usually stored in $HOME/.aws/ directory."#),
        ("logstash_format", Some(r#"false"#), r#"Enable Logstash format compatibility"#),
        ("logstash_prefix", Some(r#"logstash"#), r#"When Logstash_Format is enabled, the Index name is composed using a prefix and the date, e.g: If Logstash_Prefix is equals to 'mydata' your index will become 'mydata-YYYY.MM.DD'. The last string appended belongs to the date when the data is being generated"#),
        ("logstash_prefix_separator", Some(r#"-"#), r#"Set a separator between logstash_prefix and date."#),
        ("logstash_prefix_key", None, r#"When included: the value in the record that belongs to the key will be looked up and over-write the Logstash_Prefix for index generation. If the key/value is not found in the record then the Logstash_Prefix option will act as a fallback. Nested keys are supported through record accessor pattern"#),
        ("logstash_dateformat", Some(r#"%Y.%m.%d"#), r#"Time format (based on strftime) to generate the second part of the Index name"#),
        ("time_key", Some(r#"@timestamp"#), r#"When Logstash_Format is enabled, each record will get a new timestamp field. The Time_Key property defines the name of that field"#),
        ("time_key_format", Some(r#"%Y-%m-%dT%H:%M:%S"#), r#"When Logstash_Format is enabled, this property defines the format of the timestamp"#),
        ("time_key_nanos", Some(r#"false"#), r#"When Logstash_Format is enabled, enabling this property sends nanosecond precision timestamps"#),
        ("include_tag_key", Some(r#"false"#), r#"When enabled, it append the Tag name to the record"#),
        ("tag_key", Some(r#"flb-key"#), r#"When Include_Tag_Key is enabled, this property defines the key name for the tag"#),
        ("buffer_size", Some(r#"512k"#), r#"Specify the buffer size used to read the response from the Elasticsearch HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to 'false', otherwise the value must be according to the Unit Size specification"#),
        ("path", None, r#"Elasticsearch accepts new data on HTTP query path '/_bulk'. But it is also possible to serve Elasticsearch behind a reverse proxy on a subpath. This option defines such path on the fluent-bit side. It simply adds a path prefix in the indexing HTTP POST URI"#),
        ("pipeline", None, r#"Newer versions of Elasticsearch allows to setup filters called pipelines. This option allows to define which pipeline the database should use. For performance reasons is strongly suggested to do parsing and filtering on Fluent Bit side, avoid pipelines"#),
        ("generate_id", Some(r#"false"#), r#"When enabled, generate _id for outgoing records. This prevents duplicate records when retrying ES"#),
        ("write_operation", Some(r#"create"#), r#"Operation to use to write in bulk requests"#),
        ("id_key", None, r#"If set, _id will be the value of the key from incoming record."#),
        ("replace_dots", Some(r#"false"#), r#"When enabled, replace field name dots with underscore, required by Elasticsearch 2.0-2.3."#),
        ("current_time_index", Some(r#"false"#), r#"Use current time for index generation instead of message record"#),
        ("trace_output", Some(r#"false"#), r#"When enabled print the Elasticsearch API calls to stdout (for diag only)"#),
        ("trace_error", Some(r#"false"#), r#"When enabled print the Elasticsearch exception to stderr (for diag only)"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "File", "file", "output/file", [
        ("path", None, r#"Absolute path to store the files. This parameter is optional"#),
        ("file", None, r#"Name of the target file to write the records. If 'path' is specified, the value is prefixed"#),
        ("format", None, r#"Specify the output data format, the available options are: plain (json), csv, ltsv and template. If no value is set the outgoing data is formatted using the tag and the record in json"#),
        ("delimiter", None, r#"Set a custom delimiter for the records"#),
        ("label_delimiter", None, r#"Set a custom label delimiter, to be used with 'ltsv' format"#),
        ("template", Some(r#"{time} {message}"#), r#"Set a custom template format for the data"#),
        ("csv_column_names", Some(r#"false"#), r#"Add column names (keys) in the first line of the target file"#),
        ("mkdir", Some(r#"false"#), r#"Recursively create output directory if it does not exist. Permissions set to 0755"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Forward", "forward", "output/forward", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("time_as_integer", Some(r#"false"#), r#"Set timestamp in integer format (compat mode for old Fluentd v0.12)"#),
        ("retain_metadata_in_forward_mode", Some(r#"false"#), r#"Retain metadata when operating in forward mode"#),
        ("shared_key", None, r#"Shared key for authentication"#),
        ("self_hostname", None, r#"Hostname"#),
        ("empty_shared_key", Some(r#"false"#), r#"Set an empty shared key for authentication"#),
        ("send_options", Some(r#"false"#), r#"Send 'forward protocol options' to remote endpoint"#),
        ("require_ack_response", Some(r#"false"#), r#"Require that remote endpoint confirms data reception"#),
        ("username", Some(r#""#), r#"Username for authentication"#),
        ("password", Some(r#""#), r#"Password for authentication"#),
        ("unix_path", None, r#"Path to unix socket. It is ignored when 'upstream' property is set"#),
        ("upstream", None, r#"Path to 'upstream' configuration file (define multiple nodes)"#),
        ("tag", None, r#"Set a custom Tag for the outgoing records"#),
        ("compress", None, r#"Compression mode"#),
        ("fluentd_compat", Some(r#"false"#), r#"Send metrics and traces with Fluentd compatible format"#),
        ("add_option", None, r#"Set an extra Forward protocol option. This is an advance feature, use it only for very specific use-cases."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "HTTP", "http", "output/http", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("proxy", None, r#"Specify an HTTP Proxy. The expected format of this value is http://host:port. "#),
        ("allow_duplicated_headers", Some(r#"true"#), r#"Specify if duplicated headers are allowed or not"#),
        ("log_response_payload", Some(r#"true"#), r#"Specify if the response paylod should be logged or not"#),
        ("http_user", None, r#"Set HTTP auth user"#),
        ("http_passwd", Some(r#""#), r#"Set HTTP auth password"#),
        ("aws_auth", Some(r#"false"#), r#"Enable AWS SigV4 authentication"#),
        ("aws_service", None, r#"AWS destination service code, used by SigV4 authentication"#),
        ("aws_region", None, r#"AWS region of your service"#),
        ("aws_sts_endpoint", None, r#"Custom endpoint for the AWS STS API, used with the `aws_role_arn` option"#),
        ("aws_role_arn", None, r#"ARN of an IAM role to assume (ex. for cross account access)"#),
        ("aws_external_id", None, r#"Specify an external ID for the STS API, can be used with the `aws_role_arn` parameter if your role requires an external ID."#),
        ("aws_profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usuallystored in $HOME/.aws/ directory."#),
        ("header_tag", None, r#"Set a HTTP header which value is the Tag"#),
        ("format", Some(r#"json"#), r#"Set desired payload format: json, json_stream, json_lines, gelf or msgpack"#),
        ("json_date_format", None, r#"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch."#),
        ("json_date_key", Some(r#"date"#), r#"Specify the name of the date field in output"#),
        ("compress", None, r#"Set payload compression mechanism. Option available is 'gzip'"#),
        ("header", None, r#"Add a HTTP header key/value pair. Multiple headers can be set"#),
        ("uri", None, r#"Specify an optional HTTP URI for the target web server, e.g: /something"#),
        ("gelf_timestamp_key", None, r#"Specify the key to use for 'timestamp' in gelf format"#),
        ("gelf_host_key", None, r#"Specify the key to use for the 'host' in gelf format"#),
        ("gelf_short_message_key", None, r#"Specify the key to use as the 'short' message in gelf format"#),
        ("gelf_full_message_key", None, r#"Specify the key to use for the 'full' message in gelf format"#),
        ("gelf_level_key", None, r#"Specify the key to use for the 'level' in gelf format"#),
        ("body_key", None, r#"Specify the key which contains the body"#),
        ("headers_key", None, r#"Specify the key which contains the headers"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "InfluxDB", "influxdb", "output/influxdb", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("database", Some(r#"fluentbit"#), r#"Set the database name."#),
        ("bucket", None, r#"Specify the bucket name, used on InfluxDB API v2."#),
        ("org", Some(r#"fluent"#), r#"Set the Organization name."#),
        ("sequence_tag", None, r#"Specify the sequence tag."#),
        ("uri", None, r#"Specify a custom URI endpoint (must start with '/')."#),
        ("http_user", None, r#"HTTP Basic Auth username."#),
        ("http_passwd", Some(r#""#), r#"HTTP Basic Auth password."#),
        ("http_token", None, r#"Set InfluxDB HTTP Token API v2."#),
        ("http_header", None, r#"Add a HTTP header key/value pair. Multiple headers can be set"#),
        ("auto_tags", Some(r#"false"#), r#"Automatically tag keys where value is string."#),
        ("tag_keys", None, r#"Space separated list of keys that needs to be tagged."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "LogDNA", "logdna", "output/logdna", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("logdna_host", Some(r#"logs.logdna.com"#), r#"LogDNA Host address"#),
        ("logdna_port", Some(r#"443"#), r#"LogDNA TCP port"#),
        ("api_key", None, r#"Logdna API key"#),
        ("hostname", None, r#"Local Server or device host name"#),
        ("mac", Some(r#""#), r#"MAC address (optional)"#),
        ("ip", Some(r#""#), r#"IP address (optional)"#),
        ("tags", Some(r#""#), r#"Tags (optional)"#),
        ("file", None, r#"Name of the monitored file (optional)"#),
        ("app", Some(r#"Fluent Bit"#), r#"Name of the application generating the data (optional)"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Loki", "loki", "output/loki", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("uri", Some(r#"/loki/api/v1/push"#), r#"Specify a custom HTTP URI. It must start with forward slash."#),
        ("tenant_id", None, r#"Tenant ID used by default to push logs to Loki. If omitted or empty it assumes Loki is running in single-tenant mode and no X-Scope-OrgID header is sent."#),
        ("tenant_id_key", None, r#"If set, X-Scope-OrgID will be the value of the key from incoming record. It is useful to set X-Scode-OrgID dynamically."#),
        ("labels", None, r#"labels for API requests. If no value is set, the default label is 'job=fluent-bit'"#),
        ("structured_metadata", None, r#"optional structured metadata fields for API requests."#),
        ("auto_kubernetes_labels", Some(r#"false"#), r#"If set to true, it will add all Kubernetes labels to Loki labels."#),
        ("drop_single_key", None, r#"If set to true and only a single key remains, the log line sent to Loki will be the value of that key. If set to 'raw' and the log line is a string, the log line will be sent unquoted."#),
        ("label_keys", None, r#"Comma separated list of keys to use as stream labels."#),
        ("remove_keys", None, r#"Comma separated list of keys to remove."#),
        ("line_format", Some(r#"json"#), r#"Format to use when flattening the record to a log line. Valid values are 'json' or 'key_value'. If set to 'json' the log line sent to Loki will be the Fluent Bit record dumped as json. If set to 'key_value', the log line will be each item in the record concatenated together (separated by a single space) in the format '='."#),
        ("label_map_path", None, r#"A label map file path"#),
        ("http_user", None, r#"Set HTTP auth user"#),
        ("http_passwd", Some(r#""#), r#"Set HTTP auth password"#),
        ("buffer_size", Some(r#"512KB"#), r#"Maximum HTTP response buffer size in bytes"#),
        ("bearer_token", None, r#"Set bearer token auth"#),
        ("header", None, r#"Add a HTTP header key/value pair. Multiple headers can be set"#),
        ("compress", None, r#"Set payload compression in network transfer. Option available is 'gzip'"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Kafka", "kafka", "output/kafka", [
        ("topic_key", None, r#"Which record to use as the kafka topic."#),
        ("dynamic_topic", Some(r#"false"#), r#"Activate dynamic topics."#),
        ("format", None, r#"Set the record output format."#),
        ("message_key", None, r#"Which record key to use as the message data."#),
        ("message_key_field", None, r#"Which record key field to use as the message data."#),
        ("timestamp_key", Some(r#"@timestamp"#), r#"Set the key for the the timestamp."#),
        ("timestamp_format", None, r#"Set the format the timestamp is in."#),
        ("queue_full_retries", Some(r#"10"#), r#"Set the number of local retries to enqueue the data."#),
        ("gelf_timestamp_key", None, r#"Set the timestamp key for gelf  output."#),
        ("gelf_host_key", None, r#"Set the host key for gelf  output."#),
        ("gelf_short_message_key", None, r#"Set the short message key for gelf  output."#),
        ("gelf_full_message_key", None, r#"Set the full message key for gelf  output."#),
        ("gelf_level_key", None, r#"Set the level key for gelf  output."#),
        ("topics", None, r#"Set the kafka topics, delimited by commas."#),
        ("brokers", None, r#"Set the kafka brokers, delimited by commas."#),
        ("client_id", None, r#"Set the kafka client_id."#),
        ("group_id", None, r#"Set the kafka group_id."#),
        ("rdkafka.", None, r#"Set the kafka group_id."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Kafka REST Proxy", "kafka-rest", "output/kafka-rest-proxy", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("message_key", None, r#"Specify a message key. "#),
        ("time_key", None, r#"Specify the name of the field that holds the record timestamp. "#),
        ("topic", Some(r#"fluent-bit"#), r#"Specify the kafka topic. "#),
        ("url_path", None, r#"Specify an optional HTTP URL path for the target web server, e.g: /something"#),
        ("partition", Some(r#"-1"#), r#"Specify kafka partition number. "#),
        ("time_key_format", Some(r#"%Y-%m-%dT%H:%M:%S"#), r#"Specify the format of the timestamp. "#),
        ("include_tag_key", Some(r#"false"#), r#"Specify whether to append tag name to final record. "#),
        ("tag_key", Some(r#"_flb-key"#), r#"Specify the key name of the record if include_tag_key is enabled. "#),
        ("avro_http_header", Some(r#"false"#), r#"Specify if the format has avro header in http request"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "NATS", "nats", "output/nats", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "New Relic", "nrlogs", "output/new-relic", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("base_uri", Some(r#"https://log-api.newrelic.com/log/v1"#), r#"New Relic Host address"#),
        ("api_key", None, r#"New Relic API Key"#),
        ("license_key", None, r#"New Relic License Key"#),
        ("compress", Some(r#"gzip"#), r#"Set payload compression mechanism"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "NULL", "null", "output/null", [
        ("format", None, r#"Specifies the data format to be printed. Supported formats are msgpack json, json_lines and json_stream."#),
        ("json_date_format", None, r#"Specifies the name of the date field in output."#),
        ("json_date_key", Some(r#"date"#), r#"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "OpenSearch", "opensearch", "output/opensearch", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("index", Some(r#"fluent-bit"#), r#"Set an index name"#),
        ("type", Some(r#"_doc"#), r#"Set the document type property"#),
        ("suppress_type_name", Some(r#"false"#), r#"If true, mapping types is removed. (for v7.0.0 or later)"#),
        ("http_user", None, r#"Optional username credential for access"#),
        ("http_passwd", Some(r#""#), r#"Password for user defined in 'http_user'"#),
        ("aws_auth", Some(r#"false"#), r#"Enable AWS Sigv4 Authentication"#),
        ("aws_region", None, r#"AWS Region of your Amazon OpenSearch Service cluster"#),
        ("aws_profile", Some(r#"default"#), r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usually stored in $HOME/.aws/ directory."#),
        ("aws_sts_endpoint", None, r#"Custom endpoint for the AWS STS API, used with the AWS_Role_ARN option"#),
        ("aws_role_arn", None, r#"AWS IAM Role to assume to put records to your Amazon OpenSearch cluster"#),
        ("aws_external_id", None, r#"External ID for the AWS IAM Role specified with `aws_role_arn`"#),
        ("aws_service_name", Some(r#"es"#), r#"AWS Service Name"#),
        ("logstash_format", Some(r#"false"#), r#"Enable Logstash format compatibility"#),
        ("logstash_prefix", Some(r#"logstash"#), r#"When Logstash_Format is enabled, the Index name is composed using a prefix and the date, e.g: If Logstash_Prefix is equals to 'mydata' your index will become 'mydata-YYYY.MM.DD'. The last string appended belongs to the date when the data is being generated"#),
        ("logstash_prefix_separator", Some(r#"-"#), r#"Set a separator between logstash_prefix and date."#),
        ("logstash_prefix_key", None, r#"When included: the value in the record that belongs to the key will be looked up and over-write the Logstash_Prefix for index generation. If the key/value is not found in the record then the Logstash_Prefix option will act as a fallback. Nested keys are supported through record accessor pattern"#),
        ("logstash_dateformat", Some(r#"%Y.%m.%d"#), r#"Time format (based on strftime) to generate the second part of the Index name"#),
        ("time_key", Some(r#"@timestamp"#), r#"When Logstash_Format is enabled, each record will get a new timestamp field. The Time_Key property defines the name of that field"#),
        ("time_key_format", Some(r#"%Y-%m-%dT%H:%M:%S"#), r#"When Logstash_Format is enabled, this property defines the format of the timestamp"#),
        ("time_key_nanos", Some(r#"false"#), r#"When Logstash_Format is enabled, enabling this property sends nanosecond precision timestamps"#),
        ("include_tag_key", Some(r#"false"#), r#"When enabled, it append the Tag name to the record"#),
        ("tag_key", Some(r#"flb-key"#), r#"When Include_Tag_Key is enabled, this property defines the key name for the tag"#),
        ("buffer_size", Some(r#"512k"#), r#"Specify the buffer size used to read the response from the OpenSearch HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to 'false', otherwise the value must be according to the Unit Size specification"#),
        ("path", None, r#"OpenSearch accepts new data on HTTP query path '/_bulk'. But it is also possible to serve OpenSearch behind a reverse proxy on a subpath. This option defines such path on the fluent-bit side. It simply adds a path prefix in the indexing HTTP POST URI"#),
        ("pipeline", None, r#"OpenSearch allows to setup filters called pipelines. This option allows to define which pipeline the database should use. For performance reasons is strongly suggested to do parsing and filtering on Fluent Bit side, avoid pipelines"#),
        ("generate_id", Some(r#"false"#), r#"When enabled, generate _id for outgoing records. This prevents duplicate records when retrying"#),
        ("write_operation", Some(r#"create"#), r#"Operation to use to write in bulk requests"#),
        ("id_key", None, r#"If set, _id will be the value of the key from incoming record."#),
        ("replace_dots", Some(r#"false"#), r#"When enabled, replace field name dots with underscore."#),
        ("current_time_index", Some(r#"false"#), r#"Use current time for index generation instead of message record"#),
        ("trace_output", Some(r#"false"#), r#"When enabled print the OpenSearch API calls to stdout (for diag only)"#),
        ("trace_error", Some(r#"false"#), r#"When enabled print the OpenSearch exception to stderr (for diag only)"#),
        ("compress", None, r#"Set payload compression mechanism. Option available is 'gzip'"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Oracle Log Analytics", "oracle_log_analytics", "output/oci-logging-analytics", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("config_file_location", Some(r#""#), r#"Location of the oci config file for user api key signing"#),
        ("profile_name", Some(r#"DEFAULT"#), r#"name of the profile in the config file from which the user configs should be loaded"#),
        ("oci_config_in_record", Some(r#"false"#), r#"If true, oci_la_* configs will be read from the record"#),
        ("uri", None, r#"Set the uri for rest api request"#),
        ("oci_la_log_group_id", None, r#"log group id"#),
        ("oci_la_log_set_id", None, r#""#),
        ("oci_la_entity_id", None, r#""#),
        ("oci_la_entity_type", None, r#""#),
        ("oci_la_log_source_name", None, r#""#),
        ("oci_la_log_set_id", None, r#""#),
        ("oci_la_log_path", None, r#""#),
        ("oci_la_global_metadata", None, r#""#),
        ("oci_la_metadata", None, r#""#),
        ("namespace", None, r#"namespace in your tenancy where the log objects reside"#),
        ("proxy", None, r#"define proxy if required, in http://host:port format, supports only http protocol"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "PostgreSQL", "pgsql", "output/postgresql", [
    ]);
    add_snippet!(data, FlbSectionType::Output, "SkyWalking", "skywalking", "output/skywalking", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("auth_token", None, r#"Auth token for SkyWalking OAP"#),
        ("svc_name", Some(r#"sw-service"#), r#"Service name"#),
        ("svc_inst_name", Some(r#"fluent-bit"#), r#"Instance name"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Slack", "slack", "output/slack", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("webhook", None, r#""#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Splunk", "splunk", "output/splunk", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("compress", None, r#"Set payload compression mechanism. Option available is 'gzip'"#),
        ("http_user", None, r#"Set HTTP auth user"#),
        ("http_passwd", Some(r#""#), r#"Set HTTP auth password"#),
        ("http_buffer_size", None, r#"Specify the buffer size used to read the response from the Splunk HTTP service. This option is useful for debugging purposes where is required to read full responses, note that response size grows depending of the number of records inserted. To set an unlimited amount of memory set this value to 'false', otherwise the value must be according to the Unit Size specification"#),
        ("http_debug_bad_request", Some(r#"false"#), r#"If the HTTP server response code is 400 (bad request) and this flag is enabled, it will print the full HTTP request and response to the stdout interface. This feature is available for debugging purposes."#),
        ("event_key", None, r#"Specify the key name that will be used to send a single value as part of the record."#),
        ("event_host", None, r#"Set the host value to the event data. The value allows a record accessor pattern."#),
        ("event_source", None, r#"Set the source value to assign to the event data."#),
        ("event_sourcetype", None, r#"Set the sourcetype value to assign to the event data."#),
        ("event_sourcetype_key", None, r#"Set a record key that will populate 'sourcetype'. If the key is found, it will have precedence over the value set in 'event_sourcetype'."#),
        ("event_index", None, r#"The name of the index by which the event data is to be indexed."#),
        ("event_index_key", None, r#"Set a record key that will populate the 'index' field. If the key is found, it will have precedence over the value set in 'event_index'."#),
        ("event_field", None, r#"Set event fields for the record. This option can be set multiple times and the format is 'key_name record_accessor_pattern'."#),
        ("splunk_token", None, r#"Specify the Authentication Token for the HTTP Event Collector interface. If event metadata contains a splunk_token, it will be prioritized to use instead of this token."#),
        ("splunk_send_raw", Some(r#"off"#), r#"When enabled, the record keys and values are set in the top level of the map instead of under the event key. Refer to the Sending Raw Events section from the docs for more details to make this option work properly."#),
        ("channel", None, r#"Specify X-Splunk-Request-Channel Header for the HTTP Event Collector interface."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Stackdriver", "stackdriver", "output/stackdriver", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("google_service_credentials", None, r#"Set the path for the google service credentials file"#),
        ("metadata_server", None, r#"Set the metadata server"#),
        ("service_account_email", None, r#"Set the service account email"#),
        ("service_account_secret", None, r#"Set the service account secret"#),
        ("export_to_project_id", None, r#"Export to project id"#),
        ("project_id_key", Some(r#"logging.googleapis.com/projectId"#), r#"Set the gcp project id key"#),
        ("resource", Some(r#"global"#), r#"Set the resource"#),
        ("severity_key", Some(r#"logging.googleapis.com/severity"#), r#"Set the severity key"#),
        ("autoformat_stackdriver_trace", Some(r#"false"#), r#"Autoformat the stackdriver trace"#),
        ("trace_key", Some(r#"logging.googleapis.com/trace"#), r#"Set the trace key"#),
        ("span_id_key", Some(r#"logging.googleapis.com/spanId"#), r#"Set the span id key"#),
        ("trace_sampled_key", Some(r#"logging.googleapis.com/traceSampled"#), r#"Set the trace sampled key"#),
        ("log_name_key", Some(r#"logging.googleapis.com/logName"#), r#"Set the logname key"#),
        ("http_request_key", Some(r#"logging.googleapis.com/http_request"#), r#"Set the http request key"#),
        ("k8s_cluster_name", None, r#"Set the kubernetes cluster name"#),
        ("k8s_cluster_location", None, r#"Set the kubernetes cluster location"#),
        ("location", None, r#"Set the resource location"#),
        ("namespace", None, r#"Set the resource namespace"#),
        ("node_id", None, r#"Set the resource node id"#),
        ("job", None, r#"Set the resource job"#),
        ("task_id", None, r#"Set the resource task id"#),
        ("compress", None, r#"Set log payload compression method. Option available is 'gzip'"#),
        ("labels", None, r#"Set the labels"#),
        ("labels_key", Some(r#"logging.googleapis.com/labels"#), r#"Set the labels key"#),
        ("tag_prefix", None, r#"Set the tag prefix"#),
        ("stackdriver_agent", None, r#"Set the stackdriver agent"#),
        ("custom_k8s_regex", Some(r#"(?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$"#), r#"Set a custom kubernetes regex filter"#),
        ("resource_labels", None, r#"Set the resource labels"#),
        ("text_payload_key", None, r#"Set key for extracting text payload"#),
        ("test_log_entry_format", Some(r#"false"#), r#"Test log entry format"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Standard Output", "stdout", "output/standard-output", [
        ("format", None, r#"Specifies the data format to be printed. Supported formats are msgpack json, json_lines and json_stream."#),
        ("json_date_format", None, r#"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch."#),
        ("json_date_key", Some(r#"date"#), r#"Specifies the name of the date field in output."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Syslog", "syslog", "output/syslog", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("mode", Some(r#"udp"#), r#"Set the desired transport type, the available options are tcp and udp. If you need to use a TLS secure channel, choose 'tcp' mode here and enable the 'tls' option separately."#),
        ("syslog_format", Some(r#"rfc5424"#), r#"Specify the Syslog protocol format to use, the available options are rfc3164 and rfc5424."#),
        ("syslog_maxsize", Some(r#"0"#), r#"Set the maximum size allowed per message. The value must be only integers representing the number of bytes allowed. If no value is provided, the default size is set depending of the protocol version specified by syslog_format , rfc3164 sets max size to 1024 bytes, while rfc5424 sets the size to 2048 bytes."#),
        ("syslog_severity_key", None, r#"Specify the name of the key from the original record that contains the Syslog severity number. This configuration is optional."#),
        ("syslog_severity_preset", Some(r#"6"#), r#"Specify the preset severity number. It must be 0-7.  This configuration is optional."#),
        ("syslog_facility_key", None, r#"Specify the name of the key from the original record that contains the Syslog facility number. This configuration is optional."#),
        ("syslog_facility_preset", Some(r#"1"#), r#"Specify the preset facility number. It must be 0-23.  This configuration is optional."#),
        ("syslog_hostname_key", None, r#"Specify the key name from the original record that contains the hostname that generated the message. This configuration is optional."#),
        ("syslog_hostname_preset", None, r#"Specify the preset hostname. This configuration is optional."#),
        ("syslog_appname_key", None, r#"Specify the key name from the original record that contains the application name that generated the message. This configuration is optional."#),
        ("syslog_appname_preset", None, r#"Specify the preset appname. This configuration is optional."#),
        ("syslog_procid_key", None, r#"Specify the key name from the original record that contains the Process ID that generated the message. This configuration is optional."#),
        ("syslog_procid_preset", None, r#"Specify the preset procid.  This configuration is optional."#),
        ("syslog_msgid_key", None, r#"Specify the key name from the original record that contains the Message ID associated to the message. This configuration is optional."#),
        ("syslog_msgid_preset", None, r#"Specify the preset msgid. This configuration is optional."#),
        ("syslog_sd_key", None, r#"Specify the key name from the original record that contains the Structured Data (SD) content. If set, the value of the key must be a map.This option can be set multiple times."#),
        ("syslog_message_key", None, r#"Specify the key name that contains the message to deliver. Note that if this property is mandatory, otherwise the message will be empty."#),
        ("allow_longer_sd_id", Some(r#"false"#), r#"If true, Fluent-bit allows SD-ID that is longer than 32 characters. Such long SD-ID violates RFC 5424."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "TCP & TLS", "tcp", "output/tcp-and-tls", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("format", Some(r#"msgpack"#), r#"Specify the payload format, supported formats: msgpack, json, json_lines or json_stream."#),
        ("json_date_format", Some(r#"double"#), r#"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch."#),
        ("json_date_key", Some(r#"date"#), r#"Specify the name of the date field in output."#),
        ("raw_message_key", None, r#"use a raw message key for the message."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Treasure Data", "td", "output/treasure-data", [
        ("API", None, r#"Set the API key"#),
        ("Database", None, r#"Set the Database file"#),
        ("Table", None, r#"Set the Database Table"#),
        ("Region", None, r#"Set the Region: us or jp"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "FlowCounter", "flowcounter", "output/flowcounter", [
        ("unit", None, r#""#),
        ("event_based", Some(r#"false"#), r#""#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "GELF", "gelf", "output/gelf", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("mode", Some(r#"udp"#), r#"The protocol to use. 'tls', 'tcp' or 'udp'"#),
        ("gelf_tag_key", None, r#"Tag key name (Optional in GELF)"#),
        ("gelf_short_message_key", None, r#"A short descriptive message (MUST be set in GELF)"#),
        ("gelf_timestamp_key", None, r#"Timestamp key name (SHOULD be set in GELF)"#),
        ("gelf_host_key", None, r#"Key which its value is used as the name of the host,source or application that sent this message. (MUST be set in GELF) "#),
        ("gelf_full_message_key", None, r#"Key to use as the long message that can i.e. contain a backtrace. (Optional in GELF)"#),
        ("gelf_level_key", None, r#"Key to be used as the log level. Its value must be in standard syslog levels (between 0 and 7). (Optional in GELF)"#),
        ("packet_size", Some(r#"1420"#), r#"If transport protocol is udp, you can set the size of packets to be sent."#),
        ("compress", Some(r#"true"#), r#"If transport protocol is udp, you can set this if you want your UDP packets to be compressed."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "WebSocket", "websocket", "output/websocket", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("uri", None, r#"Specify an optional URI for the target web socket server, e.g: /something"#),
        ("format", None, r#"Set desired payload format: json, json_stream, json_lines, gelf or msgpack"#),
        ("json_date_format", Some(r#"double"#), r#"Specify the format of the date"#),
        ("json_date_key", Some(r#"date"#), r#"Specify the name of the date field in output"#),
        ("header", None, r#"Add a HTTP header key/value pair to the initial HTTP request. Multiple headers can be set"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Amazon CloudWatch", "cloudwatch_logs", "output/cloudwatch", [
        ("region", None, r#"The AWS region to send logs to"#),
        ("log_group_name", None, r#"CloudWatch Log Group Name"#),
        ("log_stream_name", None, r#"CloudWatch Log Stream Name; not compatible with `log_stream_prefix`"#),
        ("log_stream_prefix", None, r#"Prefix for CloudWatch Log Stream Name; the tag is appended to the prefix to form the stream name"#),
        ("log_group_template", None, r#"Template for CW Log Group name using record accessor syntax. Plugin falls back to the log_group_name configured if needed."#),
        ("log_stream_template", None, r#"Template for CW Log Stream name using record accessor syntax. Plugin falls back to the log_stream_name or log_stream_prefix configured if needed."#),
        ("log_key", None, r#"By default, the whole log record will be sent to CloudWatch. If you specify a key name with this option, then only the value of that key will be sent to CloudWatch. For example, if you are using the Fluentd Docker log driver, you can specify log_key log and only the log message will be sent to CloudWatch."#),
        ("extra_user_agent", None, r#"This option appends a string to the default user agent. AWS asks that you not manually set this field yourself, it is reserved for use in our vended configurations, for example, EKS Container Insights."#),
        ("log_format", None, r#"An optional parameter that can be used to tell CloudWatch the format of the data. A value of json/emf enables CloudWatch to extract custom metrics embedded in a JSON payload."#),
        ("role_arn", None, r#"ARN of an IAM role to assume (ex. for cross account access)."#),
        ("auto_create_group", Some(r#"false"#), r#"Automatically create the log group (log streams will always automatically be created)"#),
        ("auto_retry_requests", Some(r#"true"#), r#"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues."#),
        ("log_retention_days", Some(r#"0"#), r#"If set to a number greater than zero, and newly create log group's retention policy is set to this many days. Valid values are: [1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1827, 3653]"#),
        ("endpoint", None, r#"Specify a custom endpoint for the CloudWatch Logs API"#),
        ("sts_endpoint", None, r#"Specify a custom endpoint for the STS API, can be used with the role_arn parameter"#),
        ("external_id", None, r#"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID."#),
        ("metric_namespace", None, r#"Metric namespace for CloudWatch EMF logs"#),
        ("metric_dimensions", None, r#"Metric dimensions is a list of lists. If you have only one list of dimensions, put the values as a comma seperated string. If you want to put list of lists, use the list as semicolon seperated strings. If your value is 'd1,d2;d3', we will consider it as [[d1, d2],[d3]]."#),
        ("profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usually stored in $HOME/.aws/ directory."#),
        ("log_group_class", Some(r#""#), r#"Specify the log storage class. Valid values are STANDARD (default) and INFREQUENT_ACCESS."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Amazon Kinesis Data Firehose", "kinesis_firehose", "output/firehose", [
        ("region", None, r#"The AWS region of your delivery stream"#),
        ("delivery_stream", None, r#"Firehose delivery stream name"#),
        ("time_key", None, r#"Add the timestamp to the record under this key. By default the timestamp from Fluent Bit will not be added to records sent to Kinesis."#),
        ("time_key_format", None, r#"strftime compliant format string for the timestamp; for example, the default is '%Y-%m-%dT%H:%M:%S'. This option is used with time_key. "#),
        ("role_arn", None, r#"ARN of an IAM role to assume (ex. for cross account access)."#),
        ("endpoint", None, r#"Specify a custom endpoint for the Firehose API"#),
        ("sts_endpoint", None, r#"Custom endpoint for the STS API."#),
        ("external_id", None, r#"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID."#),
        ("compression", None, r#"Compression type for Firehose records. Each log record is individually compressed and sent to Firehose. 'gzip' and 'arrow' are the supported values. 'arrow' is only an available if Apache Arrow was enabled at compile time. Defaults to no compression."#),
        ("log_key", None, r#"By default, the whole log record will be sent to Firehose. If you specify a key name with this option, then only the value of that key will be sent to Firehose. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to Firehose."#),
        ("auto_retry_requests", Some(r#"true"#), r#"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues."#),
        ("profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usually stored in $HOME/.aws/ directory."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Amazon Kinesis Data Streams", "kinesis_streams", "output/kinesis", [
        ("region", None, r#"The AWS region of your kinesis stream"#),
        ("stream", None, r#"Kinesis stream name"#),
        ("time_key", None, r#"Add the timestamp to the record under this key. By default the timestamp from Fluent Bit will not be added to records sent to Kinesis."#),
        ("time_key_format", None, r#"strftime compliant format string for the timestamp; for example, the default is '%Y-%m-%dT%H:%M:%S'. This option is used with time_key. "#),
        ("role_arn", None, r#"ARN of an IAM role to assume (ex. for cross account access)."#),
        ("endpoint", None, r#"Specify a custom endpoint for the Kinesis API"#),
        ("sts_endpoint", None, r#"Custom endpoint for the STS API."#),
        ("external_id", None, r#"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID."#),
        ("log_key", None, r#"By default, the whole log record will be sent to Kinesis. If you specify a key name with this option, then only the value of that key will be sent to Kinesis. For example, if you are using the Fluentd Docker log driver, you can specify `log_key log` and only the log message will be sent to Kinesis."#),
        ("auto_retry_requests", Some(r#"true"#), r#"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues."#),
        ("profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usually stored in $HOME/.aws/ directory."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "OpenTelemetry", "opentelemetry", "output/opentelemetry", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("add_label", None, r#"Adds a custom label to the metrics use format: 'add_label name value'"#),
        ("proxy", None, r#"Specify an HTTP Proxy. The expected format of this value is http://host:port. "#),
        ("http_user", None, r#"Set HTTP auth user"#),
        ("http_passwd", Some(r#""#), r#"Set HTTP auth password"#),
        ("header", None, r#"Add a HTTP header key/value pair. Multiple headers can be set"#),
        ("metrics_uri", Some(r#"/v1/metrics"#), r#"Specify an optional HTTP URI for the target OTel endpoint."#),
        ("batch_size", Some(r#"1000"#), r#"Set the maximum number of log records to be flushed at a time"#),
        ("compress", None, r#"Set payload compression mechanism. Option available is 'gzip'"#),
        ("logs_uri", Some(r#"/v1/logs"#), r#"Specify an optional HTTP URI for the target OTel endpoint."#),
        ("logs_body_key", None, r#"Specify an optional HTTP URI for the target OTel endpoint."#),
        ("logs_body_key_attributes", Some(r#"false"#), r#"If logs_body_key is set and it matched a pattern, this option will include the remaining fields in the record as attributes."#),
        ("traces_uri", Some(r#"/v1/traces"#), r#"Specify an optional HTTP URI for the target OTel endpoint."#),
        ("log_response_payload", Some(r#"true"#), r#"Specify if the response paylod should be logged or not"#),
        ("logs_metadata_key", Some(r#"otlp"#), r#""#),
        ("logs_observed_timestamp_metadata_key", Some(r#"$ObservedTimestamp"#), r#"Specify an ObservedTimestamp key"#),
        ("logs_timestamp_metadata_key", Some(r#"$Timestamp"#), r#"Specify a Timestamp key"#),
        ("logs_severity_text_metadata_key", Some(r#"$SeverityText"#), r#"Specify a SeverityText key"#),
        ("logs_severity_number_metadata_key", Some(r#"$SeverityNumber"#), r#"Specify a SeverityNumber key"#),
        ("logs_trace_flags_metadata_key", Some(r#"$TraceFlags"#), r#"Specify a TraceFlags key"#),
        ("logs_span_id_metadata_key", Some(r#"$SpanId"#), r#"Specify a SpanId key"#),
        ("logs_trace_id_metadata_key", Some(r#"$TraceId"#), r#"Specify a TraceId key"#),
        ("logs_attributes_metadata_key", Some(r#"$Attributes"#), r#"Specify an Attributes key"#),
        ("logs_instrumentation_scope_metadata_key", Some(r#"InstrumentationScope"#), r#"Specify an InstrumentationScope key"#),
        ("logs_resource_metadata_key", Some(r#"Resource"#), r#"Specify a Resource key"#),
        ("logs_span_id_message_key", Some(r#"$SpanId"#), r#"Specify a SpanId key"#),
        ("logs_trace_id_message_key", Some(r#"$TraceId"#), r#"Specify a TraceId key"#),
        ("logs_severity_text_message_key", Some(r#"$SeverityText"#), r#"Specify a Severity Text key"#),
        ("logs_severity_number_message_key", Some(r#"$SeverityNumber"#), r#"Specify a Severity Number key"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Prometheus Exporter", "prometheus_exporter", "output/prometheus-exporter", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("add_timestamp", Some(r#"false"#), r#"Add timestamp to every metric honoring collection time."#),
        ("add_label", None, r#"TCP port for listening for HTTP connections."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Prometheus Remote Write", "prometheus_remote_write", "output/prometheus-remote-write", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("tls", Some(r#"off"#), r#"Enable or disable TLS/SSL support"#),
        ("tls.verify", Some(r#"on"#), r#"Force certificate validation"#),
        ("tls.debug", Some(r#"1"#), r#"Set TLS debug verbosity level. It accept the following values: 0 (No debug), 1 (Error), 2 (State change), 3 (Informational) and 4 Verbose"#),
        ("tls.ca_file", None, r#"Absolute path to CA certificate file"#),
        ("tls.ca_path", None, r#"Absolute path to scan for certificate files"#),
        ("tls.crt_file", None, r#"Absolute path to Certificate file"#),
        ("tls.key_file", None, r#"Absolute path to private Key file"#),
        ("tls.key_passwd", None, r#"Optional password for tls.key_file file"#),
        ("tls.vhost", None, r#"Hostname to be used for TLS SNI extension"#),
        ("tls.verify_hostname", Some(r#"off"#), r#"Enable or disable to verify hostname"#),
        ("add_label", None, r#"Adds a custom label to the metrics use format: 'add_label name value'"#),
        ("proxy", None, r#"Specify an HTTP Proxy. The expected format of this value is http://host:port. "#),
        ("http_user", None, r#"Set HTTP auth user"#),
        ("http_passwd", Some(r#""#), r#"Set HTTP auth password"#),
        ("compression", Some(r#"snappy"#), r#"Compress the payload with either snappy, gzip if set"#),
        ("aws_auth", Some(r#"false"#), r#"Enable AWS SigV4 authentication"#),
        ("aws_service", Some(r#"aps"#), r#"AWS destination service code, used by SigV4 authentication"#),
        ("aws_region", None, r#"AWS region of your service"#),
        ("aws_sts_endpoint", None, r#"Custom endpoint for the AWS STS API, used with the `aws_role_arn` option"#),
        ("aws_role_arn", None, r#"ARN of an IAM role to assume (ex. for cross account access)"#),
        ("aws_external_id", None, r#"Specify an external ID for the STS API, can be used with the `aws_role_arn` parameter if your role requires an external ID."#),
        ("aws_profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usuallystored in $HOME/.aws/ directory."#),
        ("header", None, r#"Add a HTTP header key/value pair. Multiple headers can be set"#),
        ("uri", None, r#"Specify an optional HTTP URI for the target web server, e.g: /something"#),
        ("log_response_payload", Some(r#"true"#), r#"Specify if the response paylod should be logged or not"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Amazon S3", "s3", "output/s3", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("json_date_format", None, r#"Specify the format of the date, supported formats: double, iso8601 (e.g: 2018-05-30T09:39:52.000681Z), java_sql_timestamp (e.g: 2018-05-30 09:39:52.000681, useful for AWS Athena), and epoch."#),
        ("json_date_key", Some(r#"date"#), r#"Specifies the name of the date field in output."#),
        ("total_file_size", Some(r#"100000000"#), r#"Specifies the size of files in S3. Maximum size is 50GB, minimum is 1MB"#),
        ("upload_chunk_size", Some(r#"5242880"#), r#"This plugin uses the S3 Multipart Upload API to stream data to S3, ensuring your data gets-off-the-box as quickly as possible. This parameter configures the size of each “part” in the upload. The total_file_size option configures the size of the file you will see in S3; this option determines the size of chunks uploaded until that size is reached. These chunks are temporarily stored in chunk_buffer_path until their size reaches upload_chunk_size, which point the chunk is uploaded to S3. Default: 5M, Max: 50M, Min: 5M."#),
        ("upload_timeout", Some(r#"10m"#), r#"Optionally specify a timeout for uploads. Whenever this amount of time has elapsed, Fluent Bit will complete an upload and create a new file in S3. For example, set this value to 60m and you will get a new file in S3 every hour. Default is 10m."#),
        ("bucket", None, r#"S3 bucket name."#),
        ("region", Some(r#"us-east-1"#), r#"AWS region."#),
        ("role_arn", None, r#"ARN of an IAM role to assume (ex. for cross account access)."#),
        ("endpoint", None, r#"Custom endpoint for the S3 API."#),
        ("sts_endpoint", None, r#"Custom endpoint for the STS API."#),
        ("canned_acl", None, r#"Predefined Canned ACL policy for S3 objects."#),
        ("compression", None, r#"Compression type for S3 objects. 'gzip' and 'arrow' are the supported values. 'arrow' is only an available if Apache Arrow was enabled at compile time. Defaults to no compression. If 'gzip' is selected, the Content-Encoding HTTP Header will be set to 'gzip'."#),
        ("content_type", None, r#"A standard MIME type for the S3 object; this will be set as the Content-Type HTTP header."#),
        ("store_dir", Some(r#"/tmp/fluent-bit/s3"#), r#"Directory to locally buffer data before sending. Plugin uses the S3 Multipart upload API to send data in chunks of 5 MB at a time- only a small amount of data will be locally buffered at any given point in time."#),
        ("store_dir_limit_size", None, r#"S3 plugin has its own buffering system with files in the `store_dir`. Use the `store_dir_limit_size` to limit the amount of data S3 buffers in the `store_dir` to limit disk usage. If the limit is reached, data will be discarded. Default is 0 which means unlimited."#),
        ("s3_key_format", Some(r#"/fluent-bit-logs/$TAG/%Y/%m/%d/%H/%M/%S"#), r#"Format string for keys in S3. This option supports strftime time formatters and a syntax for selecting parts of the Fluent log tag using a syntax inspired by the rewrite_tag filter. Add $TAG in the format string to insert the full log tag; add $TAG[0] to insert the first part of the tag in the s3 key. The tag is split into “parts” using the characters specified with the s3_key_format_tag_delimiters option. Add $INDEX to enable sequential indexing for file names. Adding $INDEX will prevent random string being added to end of keywhen $UUID is not provided. See the in depth examples and tutorial in the documentation."#),
        ("s3_key_format_tag_delimiters", Some(r#"."#), r#"A series of characters which will be used to split the tag into “parts” for use with the s3_key_format option. See the in depth examples and tutorial in the documentation."#),
        ("auto_retry_requests", Some(r#"true"#), r#"Immediately retry failed requests to AWS services once. This option does not affect the normal Fluent Bit retry mechanism with backoff. Instead, it enables an immediate retry with no delay for networking errors, which may help improve throughput when there are transient/random networking issues."#),
        ("use_put_object", Some(r#"false"#), r#"Use the S3 PutObject API, instead of the multipart upload API"#),
        ("send_content_md5", Some(r#"false"#), r#"Send the Content-MD5 header with object uploads, as is required when Object Lock is enabled"#),
        ("preserve_data_ordering", Some(r#"true"#), r#"Normally, when an upload request fails, there is a high chance for the last received chunk to be swapped with a later chunk, resulting in data shuffling. This feature prevents this shuffling by using a queue logic for uploads."#),
        ("log_key", None, r#"By default, the whole log record will be sent to S3. If you specify a key name with this option, then only the value of that key will be sent to S3."#),
        ("external_id", None, r#"Specify an external ID for the STS API, can be used with the role_arn parameter if your role requires an external ID."#),
        ("static_file_path", Some(r#"false"#), r#"Disables behavior where UUID string is automatically appended to end of S3 key name when $UUID is not provided in s3_key_format. $UUID, time formatters, $TAG, and other dynamic key formatters all work as expected while this feature is set to true."#),
        ("storage_class", None, r#"Specify the storage class for S3 objects. If this option is not specified, objects will be stored with the default 'STANDARD' storage class."#),
        ("profile", None, r#"AWS Profile name. AWS Profiles can be configured with AWS CLI and are usually stored in $HOME/.aws/ directory."#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Vivo Exporter", "vivo_exporter", "output/vivo-exporter", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("empty_stream_on_read", Some(r#"off"#), r#"If enabled, when an HTTP client consumes the data from a stream, the queue content will be removed"#),
        ("stream_queue_size", Some(r#"20M"#), r#"Specify the maximum queue size per stream. Each specific stream for logs, metrics and traces can hold up to 'stream_queue_size' bytes."#),
        ("http_cors_allow_origin", None, r#"Specify the value for the HTTP Access-Control-Allow-Origin header (CORS)"#),
    ]);
    add_snippet!(data, FlbSectionType::Output, "Google Chronicle", "chronicle", "output/chronicle", [
        ("host", Some(r#""#), r#"Host Address"#),
        ("port", Some(r#"0"#), r#"host Port"#),
        ("google_service_credentials", None, r#"Set the path for the google service credentials file"#),
        ("service_account_email", None, r#"Set the service account email"#),
        ("service_account_secret", None, r#"Set the service account secret"#),
        ("project_id", None, r#"Set the project id"#),
        ("customer_id", None, r#"Set the customer id"#),
        ("log_type", None, r#"Set the log type"#),
        ("region", None, r#"Set the region"#),
        ("log_key", None, r#"Set the log key"#),
    ]);

    data
});
